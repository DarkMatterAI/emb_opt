[
  {
    "objectID": "filter.html",
    "href": "filter.html",
    "title": "Filter",
    "section": "",
    "text": "The Filter step optionally removes Item results based on some boolean criteria. This can be used to remove undesirable results prior to scoring and updating. The filter step is formalized by the FilterFunction schema, which maps inputs List[Item] to outputs List[FilterResponse]\nThe FilterModule manages execution of a FilterFunction. The FilterModule gathers valid items, sends them to the FilterFunction, and processes the results.\n\nsource\n\nFilterModule\n\n FilterModule (function:Callable[[List[emb_opt.schemas.Item]],List[emb_opt\n               .schemas.FilterResponse]])\n\nModule - module base class\nGiven an input Batch, the Module: 1. gathers inputs to the function 2. executes the function 3. validates the results of the function with output_schema 4. scatters results back into the Batch\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunction\ntyping.Callable[[typing.List[emb_opt.schemas.Item]], typing.List[emb_opt.schemas.FilterResponse]]\nfilter function\n\n\n\n\ndef build_batch(cutoff=10.5):\n    d_emb = 128\n    n_emb = 100\n    np.random.seed(42)\n    \n    embeddings = np.random.randn(n_emb+1, d_emb)\n    query = Query.from_minimal(embedding=embeddings[-1])\n    results = [Item.from_minimal(id=i, embedding=embeddings[i]) for i in range(n_emb)]\n    query.add_query_results(results)\n    batch = Batch(queries=[query])\n    expected_failures = [i.id for i in results if np.linalg.norm(i.embedding)&gt;=cutoff]\n    return batch, expected_failures\n\nclass NormFilter():\n    def __init__(self, cutoff=10.5):\n        self.cutoff = cutoff\n        \n    def __call__(self, inputs: List[Item]) -&gt; List[FilterResponse]:\n        \n        embeddings = np.array([i.embedding for i in inputs])\n        norms = np.linalg.norm(embeddings, axis=-1)\n        results = [FilterResponse(valid=i&lt;self.cutoff, data={'norm':i}) for i in norms]\n        return results\n    \nfilter_func = NormFilter()\nfilter_module = FilterModule(filter_func)\n\nbatch, fails = build_batch()\nbatch2 = filter_module(batch)\n\nassert len(batch2.flatten_query_results(skip_removed=True)[1]) == len(batch[0])-len(fails)\n\nfor i in range(len(batch[0])):\n    result = batch[0][i]\n    if i in fails:\n        assert result.internal.removed\n    else:\n        assert not result.internal.removed\n        \n    assert result.internal.parent_id == batch[0].id\n    \nbatch, fails = build_batch()\nfilter_func = NormFilter(cutoff=-1)\nfilter_module = FilterModule(filter_func)\nbatch, fails = build_batch(cutoff=-1)\nbatch2 = filter_module(batch)\n\nassert batch2[0].internal.removed\n\n\nsource\n\n\nFilterPlugin\n\n FilterPlugin ()\n\nFilterPlugin - documentation for plugin functions to FilterFunction\nA valid FilterFunction is any function that maps List[Item] to List[FilterResponse]. The inputs will be given as Item objects. The outputs can be either a list of FilterResponse objects or a list of valid json dictionaries that match the FilterResponse schema.\nItem schema:\n{     'id' : Optional[Union[str, int]]     'item' : Optional[Any],     'embedding' : List[float],     'score' : None, # will be None at this stage     'data' : Optional[Dict], }\nInput schema:\nList[Item]\nFilterResponse schema:\n{     'valid' : bool,     'data' : Optional[Dict], }\nOutput schema:\nList[FilterResponse]\nThe CompositeFilterPlugin can be used to chain together a list of valid FilterFunction\n\nsource\n\n\nCompositeFilterPlugin\n\n CompositeFilterPlugin (functions:List[Callable[[List[emb_opt.schemas.Item\n                        ]],List[emb_opt.schemas.FilterResponse]]])\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunctions\ntyping.List[typing.Callable[[typing.List[emb_opt.schemas.Item]], typing.List[emb_opt.schemas.FilterResponse]]]\nlist of filter functions\n\n\n\n\ndef build_batch(cutoff=10.5):\n    d_emb = 128\n    n_emb = 100\n    np.random.seed(42)\n    \n    embeddings = np.random.randn(n_emb+1, d_emb)\n    query = Query.from_minimal(embedding=embeddings[-1])\n    results = [Item.from_minimal(id=i, embedding=embeddings[i]) for i in range(n_emb)]\n    query.add_query_results(results)\n    batch = Batch(queries=[query])\n    return batch\n\ndef norm_filter(inputs: List[Item], cutoff: float=10.5) -&gt; List[FilterResponse]:\n    embeddings = np.array([i.embedding for i in inputs])\n    norms = np.linalg.norm(embeddings, axis=-1)\n    results = [FilterResponse(valid=i&lt;cutoff, data={'norm':i}) for i in norms]\n    return results\n\ndef sum_filter(inputs: List[Item], cutoff: float=0.0) -&gt; List[FilterResponse]:\n    embeddings = np.array([i.embedding for i in inputs])\n    sums = embeddings.sum(-1)\n    results = [FilterResponse(valid=i&gt;cutoff, data={'sum':i}) for i in sums]\n    return results\n\nfilter_func = CompositeFilterPlugin([norm_filter, sum_filter])\n\nfilter_module = FilterModule(filter_func)\n\nbatch = build_batch()\nbatch2 = filter_module(batch)",
    "crumbs": [
      "Filter"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html",
    "href": "tutorials/basic_tutorial.html",
    "title": "Basic Tutorial",
    "section": "",
    "text": "The goal of emb_opt is to use hill climbing to find high scoring items (defined by some score function) in a vector database without exhaustive screening of the database itself.\nemb_opt takes advantage of the inherent structure present in embedding spaces. Starting with random points, we can use reinforcement learning to traverse the vector space to move from low scoring items to high scoring items.\nThis notebook gives an overview of the main abstractions in emb_opt and how they fit together.\n\nfrom emb_opt.imports import *\nfrom emb_opt.schemas import Query, Item, Batch\n\nimport string\nfrom datasets import Dataset\n\nimport matplotlib.pyplot as plt\n\n/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#overview",
    "href": "tutorials/basic_tutorial.html#overview",
    "title": "Basic Tutorial",
    "section": "",
    "text": "The goal of emb_opt is to use hill climbing to find high scoring items (defined by some score function) in a vector database without exhaustive screening of the database itself.\nemb_opt takes advantage of the inherent structure present in embedding spaces. Starting with random points, we can use reinforcement learning to traverse the vector space to move from low scoring items to high scoring items.\nThis notebook gives an overview of the main abstractions in emb_opt and how they fit together.\n\nfrom emb_opt.imports import *\nfrom emb_opt.schemas import Query, Item, Batch\n\nimport string\nfrom datasets import Dataset\n\nimport matplotlib.pyplot as plt\n\n/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#setup",
    "href": "tutorials/basic_tutorial.html#setup",
    "title": "Basic Tutorial",
    "section": "Setup",
    "text": "Setup\nFirst we need a vector database to query. Since this is a simple example notebook, we will work off a set of random vectors.\nWe will use the Datasets library to hold our vectors and build our index.\nOur dataset will contain the following columns: * index - the ID of each item * item - the specific thing each embedding represents (in this case a random text string) * rand - a random number we will use for filtering * embedding - the embedding itself\n\nn_vectors = 10000\nsize = 64\n\nnp.random.seed(42)\nvectors = np.random.randn(n_vectors, size)\n\nvector_data = [{\n                'index' : i,\n                'item' : ''.join(np.random.choice([i for i in string.ascii_lowercase], size=10).tolist()),\n                'rand' : np.random.rand(),\n                'embedding' : vectors[i]\n            } for i in range(n_vectors)]\n\nvector_dataset = Dataset.from_list(vector_data)\nvector_dataset.add_faiss_index('embedding')\n\n100%|█████████████████████████████████████████| 10/10 [00:00&lt;00:00, 1101.59it/s]\n\n\nDataset({\n    features: ['index', 'item', 'rand', 'embedding'],\n    num_rows: 10000\n})",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#data-models",
    "href": "tutorials/basic_tutorial.html#data-models",
    "title": "Basic Tutorial",
    "section": "Data Models",
    "text": "Data Models\nemb_opt uses Pydantic data models to standardize inputs and outputs of various modules. Here we introduce several relevant data models.\n\nItem Data Model\nThe Item data model is the standard format for something returned from a vector database\nclass Item(BaseModel):\n    id: Optional[Union[str, int]] # id/index of item\n    item: Optional[Any]           # the item itself\n    embedding: List[float]        # embedding representing the item\n    score: Optional[float]        # item score\n    data: Optional[dict]          # any other associated data\n\nItem.id is expected to be unique\nItem.item is the object associated with the embedding. This field is optional, allowing for working purely in embedding space\nItem.embedding is the item’s embedding\nItem.score is the item’s score, which will be computed at a later point\nItem.data is a dictionary of any other associated data\n\n\n\nQuery Data Model\nThe Query data model is the standard format for making a query to a vector database\nclass Query(BaseModel, extra='allow'):\n    item: Optional[Any]                  # Optional item associated with query\n    embedding: List[float]               # Query embedding\n    data: Optional[dict]                 # data associated with the query\n    query_results: Optional[list[Item]]  # list of `Item` query results\n\nQuery.item is the object associated with the embedding. If the Query is created from a specific Item, this field will be populated with Item.item. If the Query is a pure embedding query (ie created by averaging embeddings or a similar embedding-space process), Query.item will be None\nQuery.embedding is the query embedding\nQuery.data is a dictionary of any data related to the query\nQuery.query_results is a list of Item results returned by the query",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#data-source",
    "href": "tutorials/basic_tutorial.html#data-source",
    "title": "Basic Tutorial",
    "section": "Data Source",
    "text": "Data Source\nNow we need to convert our vector database into a format compatible with emb_opt. To integrate a data source, it must be compatible with the DataSourceFunction data model:\nDataSourceFunction = Callable[List[Query], List[DataSourceResponse]]\n\nclass DataSourceResponse(BaseModel):\n    valid: bool                # if input `Query` was valid (if False, associated `Query` is removed)\n    data: Optional[Dict]       # optional dict of data associated with the query\n    query_results: List[Item]  # list of `Item` results\nFor more details, see the DataSourcePlugin documentation.\nOur dataset is a Huggingface dataset, so we will use the HugggingfaceDataPlugin class from the plugins library. emb_opt currently supports data plugins for faiss, Qdrant, and Huggingface Datasets.\n\nfrom emb_opt.plugins.huggingface import HugggingfaceDataPlugin\n\n\nk = 10 # return 10 items per query\n\ndata_plugin = HugggingfaceDataPlugin(k=k, \n                                     dataset=vector_dataset, \n                                     index_name='embedding', \n                                     item_key='item', \n                                     id_key='index', \n                                     distance_cutoff=None\n                                    )",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#score-function",
    "href": "tutorials/basic_tutorial.html#score-function",
    "title": "Basic Tutorial",
    "section": "Score Function",
    "text": "Score Function\nThis is the score we want to maximize. To be compatible with emb_opt, the score function must be compatible with the ScoreFunction data model:\nScoreFunction = Callable[List[Item], List[ScoreResponse]]\n\nclass ScoreResponse(BaseModel):\n    valid: bool             # if the input `Item` is valid (if False, associated `Item` is removed)\n    score: Optional[float]  # the score of the input `Item`. Can be `None` if `valid=False`\n    data: Optional[Dict]    # optional dict of data associated with the score response\nFor more details, see the ScorePlugin documentation.\nFor our score function, we will use a function that has a maxima at the point [0.75, 0.75, ... 0.75] and a score value that decays exponentially with respect to distance from that point.\nscore_embeddings will compute the score using numpy arrays, and score_plugin will wrap this function to make the inputs and outputs compatible with the ScoreFunction data model\n\nfrom emb_opt.schemas import ScoreResponse\n\n\ndef score_embeddings(embeddings: np.ndarray, sigma: float=5.) -&gt; np.ndarray:\n    target_point = np.ones(embeddings.shape[1])*.75\n    \n    distances = np.linalg.norm(embeddings - target_point, axis=1) / np.sqrt(embeddings.shape[1])\n    \n    scores = np.exp(-0.5 * (distances/sigma)**2)\n        \n    return scores\n\ndef score_plugin(inputs: List[Item]) -&gt; List[ScoreResponse]:\n    embeddings = np.array([i.embedding for i in inputs])\n    scores = score_embeddings(embeddings)    \n    results = [ScoreResponse(valid=True, score=i, data=None) for i in scores]\n    return results",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#filter-function",
    "href": "tutorials/basic_tutorial.html#filter-function",
    "title": "Basic Tutorial",
    "section": "Filter Function",
    "text": "Filter Function\nIn some cases, we may wish to filter scores prior to scoring. We can implement filtering directly in the score function using the ScoreResponse.valid field, but it may be more efficient to separate the steps as filtering before scoring allows us to removed items that fail the filter and rebatch only valid items for scoring.\nFilter functions are defined by the FilterFunction data model\nFilterFunction = Callable[List[Item], List[FilterResponse]]\n\nclass FilterResponse(BaseModel):\n    valid: bool           # if the input `Item` is valid (if False, associated `Item` is removed)\n    data: Optional[Dict]  # optional dict of data associated with the filter response\nFor more details, see the FilterPlugin documentation.\nWhen we generated our dataset, we added a rand column containing a random value between 0 and 1. As an example filter function, we will reject any item where rand &gt;= 0.9\n\nfrom emb_opt.schemas import FilterResponse\n\n\ndef filter_plugin(inputs: List[Item]) -&gt; List[FilterResponse]:\n    return [FilterResponse(valid=i.data['rand']&lt;0.9, data={'rand':i.data['rand']}) for i in inputs]",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#update-function",
    "href": "tutorials/basic_tutorial.html#update-function",
    "title": "Basic Tutorial",
    "section": "Update Function",
    "text": "Update Function\nAfter filtering and scoring, every Query will have a list of query result Item objects that have been scored. The update step uses this information to generate a new query.\nWe can use any update method that corresponds to the UpdateFunction data model\nUpdateFunction = Callable[List[Query], List[UpdateResponse]]\n\nclass UpdateResponse(BaseModel):\n    query: Query                # the new `Query`\n    parent_id: Optional[str]    # optional parent query ID, used for tracking query progression\nFor this example, we will use reinforcement learning to estimate the gradient of the query based on the scored results, and use gradient descent to update.\n\nfrom emb_opt.update import RLUpdate\n\n\nupdate_plugin = RLUpdate(lrs=[.25], distance_penalty=0.)",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#runner",
    "href": "tutorials/basic_tutorial.html#runner",
    "title": "Basic Tutorial",
    "section": "Runner",
    "text": "Runner\nThe Runner class holds all these objects together and executes the search. The search algorithm follows: 1. Query the data source 2. Filter results 3. Score results 4. Use scores to create new queries\n\nfrom emb_opt.runner import Runner\n\n\n# the `None` keyword corresponds to a prune plugin, which is explained in another tutorial\nrunner = Runner(data_plugin, filter_plugin, score_plugin, None, update_plugin)",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#initialize",
    "href": "tutorials/basic_tutorial.html#initialize",
    "title": "Basic Tutorial",
    "section": "Initialize",
    "text": "Initialize\nWe start with 5 random query embeddings. These are converted into a Batch using the build_batch_from_embeddings function. The Batch data model holds a list of embeddings, with several helper functions for iterating over queries and query results\n\nfrom emb_opt.utils import build_batch_from_embeddings\n\n\nn_queries = 5\nnp.random.seed(40)\ninitial_queries = np.random.randn(n_queries, size)\n\ninput_batch = build_batch_from_embeddings(initial_queries)",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#run",
    "href": "tutorials/basic_tutorial.html#run",
    "title": "Basic Tutorial",
    "section": "Run",
    "text": "Run\nRunning the input batch generates output_batch, the result of the final update step, and search_log, which contains all the found results\n\noutput_batch, search_log = runner.search(input_batch, 15)\n\n0 0.97 0.97 0.97 0.97 0.98\n1 0.98 0.97 0.98 0.97 0.98\n2 0.98 0.97 0.98 0.97 0.98\n3 0.98 0.98 0.98 0.97 0.98\n4 0.98 0.98 0.98 0.97 0.98\n5 0.98 0.98 0.98 0.98 0.98\n6 0.98 0.98 0.98 0.98 0.98\n7 0.98 0.98 0.98 0.98 0.98\n8 0.98 0.98 0.98 0.98 0.98\n9 0.98 0.98 0.98 0.98 0.98\n10 0.98 0.98 0.98 0.98 0.98\n11 0.98 0.98 0.98 0.98 0.98\n12 0.98 0.98 0.98 0.98 0.98\n13 0.98 0.98 0.98 0.98 0.98\n14 0.98 0.98 0.98 0.98 0.98",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#eval",
    "href": "tutorials/basic_tutorial.html#eval",
    "title": "Basic Tutorial",
    "section": "Eval",
    "text": "Eval\nNow we want to evaluate the outputs. First we need the ground truth from our dataset\n\nground_truth = vector_dataset.map(lambda row: \n                            {'score' : float(score_embeddings(np.array(row['embedding'])[None]))})\nground_truth = ground_truth.filter(lambda row: row['rand']&lt;0.9)\nground_truth = ground_truth.sort('score', reverse=True)\n\n                                                                                \n\n\nNow we’ll grab the results from the search_log\n\nresults = search_log.compile_results(skip_removed=False)\nlen(results)\n\n186\n\n\nThe search log contains 186 items. This represents a 1.8% screening of the total search space.\nNow we’ll look at the fraction of top k ground truth items recovered.\nWe can see the queries successfully found the top scoring result, but only 40% of the top 10 results. Methods to tune performance are discussed in other tutorials.\n\nk_vals = [1, 5, 10, 50]\nrecovered_ids = set([i['id'] for i in results])\nmetrics = []\n\nfor k in k_vals:\n    gt_idxs = set(ground_truth[:k]['index'])\n    percent_recovered = len(gt_idxs.intersection(recovered_ids))/k\n    metrics.append(percent_recovered)\n    \nmetrics\n\n[1.0, 0.4, 0.4, 0.2]\n\n\nThe search_log also contains a query_tree which maps out the progression of queries through the search. We can use this to reconstruct trajectories of the average score over the search\n\nfor final_node in search_log.query_tree.leaf_nodes():\n    max_scores = []\n    \n    current = final_node\n    while current:\n        max_scores.append(current.mean_score)\n        current = current.parent\n        \n    plt.plot(max_scores[::-1], color='b', alpha=0.5)",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#coments-on-results",
    "href": "tutorials/basic_tutorial.html#coments-on-results",
    "title": "Basic Tutorial",
    "section": "Coments on results",
    "text": "Coments on results\nThe compiled results returned 186 items. We used 5 query vectors with 10 items returned per query over 15 iterations. This implies 750 total items should have been returned. The reduction from 750 total items to 186 is a result of the query vectors converging to the same location and returning duplicate items.\nThe degree of duplicate results depends on: * size of the vector database * how query vectors are initialized * how many local optima exist for your score function * how many iterations are run * how many results are returned per query",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/basic_tutorial.html#conclusions",
    "href": "tutorials/basic_tutorial.html#conclusions",
    "title": "Basic Tutorial",
    "section": "Conclusions",
    "text": "Conclusions\nThis notebook gave a brief overview of the main components of emb_opt. We used hill climbing to find the top scoring item in a vector dataset while only evaluating 1.8% of the total search space.\nThe next set of tutorials will go over advanced usage of the library and methods to tune performance.",
    "crumbs": [
      "tutorials",
      "Basic Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html",
    "href": "tutorials/advanced_tutorial.html",
    "title": "Advanced Tutorial",
    "section": "",
    "text": "The previous tutorial gave an overview of the main abstractions in emb_opt for basic hill climbing. This notebook goes over some more advanced query updating strategies\n\nfrom emb_opt.imports import *\nfrom emb_opt.schemas import Query, Item, Batch, ScoreResponse, FilterResponse\nfrom emb_opt.plugins.huggingface import HugggingfaceDataPlugin\nfrom emb_opt.runner import Runner\nfrom emb_opt.utils import build_batch_from_embeddings\n\nimport string\nfrom datasets import Dataset\n\nimport matplotlib.pyplot as plt\n\n/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html#overview",
    "href": "tutorials/advanced_tutorial.html#overview",
    "title": "Advanced Tutorial",
    "section": "",
    "text": "The previous tutorial gave an overview of the main abstractions in emb_opt for basic hill climbing. This notebook goes over some more advanced query updating strategies\n\nfrom emb_opt.imports import *\nfrom emb_opt.schemas import Query, Item, Batch, ScoreResponse, FilterResponse\nfrom emb_opt.plugins.huggingface import HugggingfaceDataPlugin\nfrom emb_opt.runner import Runner\nfrom emb_opt.utils import build_batch_from_embeddings\n\nimport string\nfrom datasets import Dataset\n\nimport matplotlib.pyplot as plt\n\n/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html#setup",
    "href": "tutorials/advanced_tutorial.html#setup",
    "title": "Advanced Tutorial",
    "section": "Setup",
    "text": "Setup\nTo start, we’ll set up the same dataset, filter function and score function from the previous notebook\n\ndef get_dataset(n_vectors: int=10000, size: int=64):\n    \n    np.random.seed(42)\n    vectors = np.random.randn(n_vectors, size)\n\n    vector_data = [{\n                    'index' : i,\n                    'item' : ''.join(np.random.choice([i for i in string.ascii_lowercase], size=10).tolist()),\n                    'rand' : np.random.rand(),\n                    'embedding' : vectors[i]\n                } for i in range(n_vectors)]\n\n    vector_dataset = Dataset.from_list(vector_data)\n    vector_dataset.add_faiss_index('embedding')\n    \n    return vector_dataset\n\ndef get_data_plugin(dataset: Dataset, k: int=10, distance_cutoff: Optional[float]=None):\n    data_plugin = HugggingfaceDataPlugin(k=k, \n                                         dataset=dataset, \n                                         index_name='embedding', \n                                         item_key='item', \n                                         id_key='index', \n                                         distance_cutoff=distance_cutoff\n                                        )\n    return data_plugin\n\n\ndef score_embeddings(embeddings: np.ndarray, sigma: float=5.) -&gt; np.ndarray:\n    target_point = np.ones(embeddings.shape[1])*.75\n    \n    distances = np.linalg.norm(embeddings - target_point, axis=1) / np.sqrt(embeddings.shape[1])\n    \n    scores = np.exp(-0.5 * (distances/sigma)**2)\n        \n    return scores\n\ndef score_plugin(inputs: List[Item]) -&gt; List[ScoreResponse]:\n    embeddings = np.array([i.embedding for i in inputs])\n    scores = score_embeddings(embeddings)    \n    results = [ScoreResponse(valid=True, score=i, data=None) for i in scores]\n    return results\n\n\ndef filter_plugin(inputs: List[Item]) -&gt; List[FilterResponse]:\n    return [FilterResponse(valid=i.data['rand']&lt;0.9, data={'rand':i.data['rand']}) for i in inputs]\n\n\ndef get_input_batch(n_queries: int=5, size: int=64):\n    np.random.seed(40)\n    initial_queries = np.random.randn(n_queries, size)\n    input_batch = build_batch_from_embeddings(initial_queries)\n    return input_batch\n\n\ndataset = get_dataset(n_vectors=10000)\n\n100%|█████████████████████████████████████████| 10/10 [00:00&lt;00:00, 1162.11it/s]\n\n\n\nground_truth = dataset.map(lambda row: \n                            {'score' : float(score_embeddings(np.array(row['embedding'])[None]))})\nground_truth = ground_truth.filter(lambda row: row['rand']&lt;0.9)\nground_truth = ground_truth.sort('score', reverse=True)\n\n                                                                                \n\n\n\ndata_plugin = get_data_plugin(dataset, k=10)",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html#batch-creation",
    "href": "tutorials/advanced_tutorial.html#batch-creation",
    "title": "Advanced Tutorial",
    "section": "Batch Creation",
    "text": "Batch Creation\nHere we create a Batch of starting queries\n\nbatch = get_input_batch()",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html#modules",
    "href": "tutorials/advanced_tutorial.html#modules",
    "title": "Advanced Tutorial",
    "section": "Modules",
    "text": "Modules\nNow we want to take our batch and run a database query, filter function query, and score function query. The easiest way to do this is wrapping our various functions in their corresponding Module class. Module classes handle gathering the correct inputs from a Batch and processing the outputs.\nInternally, the Runner class is organized as:\nRunner\n    DataSourceModule\n        DataSourceFunction\n    FilterModule\n        FilterFunction\n    ScoreModule\n        ScoreFunction\n    PruneModule\n        PruneFunction\n    UpdateModule\n        UpdateFunction\n\nfrom emb_opt.data_source import DataSourceModule\nfrom emb_opt.filter import FilterModule\nfrom emb_opt.score import ScoreModule\n\n\ndata_module = DataSourceModule(data_plugin)\nfilter_module = FilterModule(filter_plugin)\nscore_module = ScoreModule(score_plugin)\n\n\nbatch = data_module(batch)\nbatch = filter_module(batch)\nbatch = score_module(batch)\n\nNow we have a batch with filtered and scored items. We can verify this by iterating through query results within the batch.\n\nfor _, query_result in batch.enumerate_query_results(skip_removed=False):\n    print(f'removed: {query_result.internal.removed}, score: {query_result.score if query_result.score else None}')\n\nremoved: False, score: 0.9716392517882978\nremoved: False, score: 0.9735146862660943\nremoved: False, score: 0.9778528690676259\nremoved: False, score: 0.9724728791637264\nremoved: False, score: 0.9782676902487691\nremoved: False, score: 0.9737127204592259\nremoved: False, score: 0.9804037820920216\nremoved: False, score: 0.9753909225654782\nremoved: False, score: 0.9694685722987455\nremoved: True, score: None\nremoved: True, score: None\nremoved: False, score: 0.9726323141284812\nremoved: False, score: 0.9692556815122584\nremoved: False, score: 0.9745647899839788\nremoved: False, score: 0.9629961168102679\nremoved: False, score: 0.9701815738190763\nremoved: False, score: 0.97486965277314\nremoved: False, score: 0.9711548702455153\nremoved: False, score: 0.9694112397200653\nremoved: True, score: None\nremoved: False, score: 0.9759289815073339\nremoved: False, score: 0.9773133941752246\nremoved: False, score: 0.9739640115726699\nremoved: False, score: 0.9623959805518694\nremoved: False, score: 0.9779404070528597\nremoved: False, score: 0.9741377917753992\nremoved: False, score: 0.9736229367471343\nremoved: False, score: 0.973143133055708\nremoved: False, score: 0.9710786179918796\nremoved: False, score: 0.9748908586503796\nremoved: False, score: 0.9694946512313679\nremoved: False, score: 0.9731516805295557\nremoved: False, score: 0.9779732635661752\nremoved: True, score: None\nremoved: False, score: 0.9672662450927711\nremoved: False, score: 0.9710809731569571\nremoved: False, score: 0.9757451096566242\nremoved: True, score: None\nremoved: False, score: 0.9734810008615996\nremoved: False, score: 0.9638052662639214\nremoved: False, score: 0.970608709429522\nremoved: False, score: 0.9755166555429822\nremoved: False, score: 0.9744431540375411\nremoved: False, score: 0.978210361238593\nremoved: False, score: 0.9736627269393461\nremoved: False, score: 0.9712608487343193\nremoved: False, score: 0.9779411824359353\nremoved: False, score: 0.9780864007137536\nremoved: False, score: 0.9759495684365427\nremoved: False, score: 0.9774162832279337",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html#update-function",
    "href": "tutorials/advanced_tutorial.html#update-function",
    "title": "Advanced Tutorial",
    "section": "Update Function",
    "text": "Update Function\nPreviously, we used the RLUpdate class to update our batch like so:\n\nfrom emb_opt.update import RLUpdate, UpdateModule\n\n\nupdate_plugin = RLUpdate(lrs=[.25], distance_penalty=0.)\nupdate_module = UpdateModule(update_plugin)\nnew_batch = update_module(batch)\n\nWe can see the new batch contains the same number of new queries as the old batch\n\nassert len(new_batch)==len(batch)\nprint(len(new_batch), len(batch))\n\n5 5\n\n\n\nBranching Updates\nemb_opt supports update methods that generate a variable number of output queries. We can pass more learning rates to RLUpdate to take different step sizes based on our gradient.\n\nupdate_plugin = RLUpdate(lrs=[0.1, .25, 0.5, 1.], distance_penalty=0.)\nupdate_module = UpdateModule(update_plugin)\nnew_batch = update_module(batch)\nprint(len(new_batch), len(batch))\n\n20 5\n\n\nWe can also use the TopKDiscreteUpdate to select k new queries from each parent query based on top scoring items\n\nfrom emb_opt.update import TopKDiscreteUpdate\n\n\nupdate_plugin = TopKDiscreteUpdate(k=4)\nupdate_module = UpdateModule(update_plugin)\nnew_batch = update_module(batch)\nprint(len(new_batch), len(batch))\n\n20 5\n\n\nThis allows for a lot of flexibility in how updates are executed, but creates a problem of managing the total number of queries. If the number of queries increases by a constant multiple, the total number of queries quickly gets out of hand and inefficient. We can manage this with the Prune step",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html#prune-function",
    "href": "tutorials/advanced_tutorial.html#prune-function",
    "title": "Advanced Tutorial",
    "section": "Prune Function",
    "text": "Prune Function\nA Prune function exists to prune queries to control the total query number. Pruning happens after Filter and Score steps, but before the Update step. This allows us to use score information to inform which queries we prune.\nWe can use any prune method that corresponds to the PruneFunction data model\nPruneFunction = Callable[List[Query], List[PruneResponse]]\n\nclass PruneResponse(BaseModel):\n    valid: bool           # if the input `Query` item is valid (if False, the associated `Query` is removed)\n    data: Optional[Dict]  # optional dict of data associated with the prune response\n\nfrom emb_opt.prune import TopKPrune, PruneModule\n\n\nprune_plugin = TopKPrune(k=1, score_agg='mean', group_by='collection_id')\nprune_module = PruneModule(prune_plugin)\n\n\n# starting batch size\nprint(len(list(batch.valid_queries())))\n\n# create new batch with more queries\nnew_batch = update_module(batch)\nprint(len(list(new_batch.valid_queries())))\n\n# query, filter, score\nnew_batch = data_module(new_batch)\nnew_batch = filter_module(new_batch)\nnew_batch = score_module(new_batch)\nprint(len(list(new_batch.valid_queries())))\n\n# prune\nnew_batch = prune_module(new_batch)\nprint(len(list(new_batch.valid_queries())))\n\n5\n20\n20\n5",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html#running-with-branching-and-pruning",
    "href": "tutorials/advanced_tutorial.html#running-with-branching-and-pruning",
    "title": "Advanced Tutorial",
    "section": "Running with branching and pruning",
    "text": "Running with branching and pruning\nHere we show running with branching and pruning. We use an RL update with several learning rates, and prune back to a single update by mean score. This allows us to converge to the top 1 result with fewer iterations\n\nupdate_plugin = RLUpdate(lrs=[0.25, 1., 5.], distance_penalty=0.)\nprune_func = TopKPrune(k=1, score_agg='mean', group_by='collection_id')\n\n\nrunner = Runner(data_plugin, filter_plugin, score_plugin, prune_plugin, update_plugin)\n\n\ninput_batch = get_input_batch(n_queries=3)\n\n\noutput_batch, search_log = runner.search(input_batch, 3)\n\n0 0.97 0.97 0.97\n1 0.98 0.97 0.98\n2 0.98 0.98 0.98\n\n\nWe found the same top 1 result with reduced coverage of the full dataset\n\nresults = search_log.compile_results(skip_removed=False)\nlen(results)\n\n112\n\n\n\nk_vals = [1, 5, 10, 50]\nrecovered_ids = set([i['id'] for i in results])\nmetrics = []\n\nfor k in k_vals:\n    gt_idxs = set(ground_truth[:k]['index'])\n    percent_recovered = len(gt_idxs.intersection(recovered_ids))/k\n    metrics.append(percent_recovered)\n    \nmetrics\n\n[1.0, 0.4, 0.3, 0.12]\n\n\nBy plotting trajectories, we can see the branching and pruning\n\nfor final_node in search_log.query_tree.leaf_nodes(include_removed=True):\n    mean_scores = []\n    \n    current = final_node\n    while current:\n        mean_scores.append(current.mean_score)\n        current = current.parent\n        \n    plt.plot(mean_scores[::-1], color='b', alpha=0.5)",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "tutorials/advanced_tutorial.html#gradient-based-queries",
    "href": "tutorials/advanced_tutorial.html#gradient-based-queries",
    "title": "Advanced Tutorial",
    "section": "Gradient Based Queries",
    "text": "Gradient Based Queries\nSo far, we have used gradients to Update our queries. We can also use gradients in the query itself. We take a query embedding and project it along the score gradient, creating multiple queries.\nFor this to work, we need to know the gradient of the query embedding at query time. This is managed by the classes UpdatePluginGradientWrapper and DataPluginGradWrapper.\nUpdatePluginGradientWrapper takes a new query and estimates its gradient using scored items from the new query’s parent query. The gradient values are stored in the new query’s data['_score_grad'] attribute.\nDataPluginGradWrapper then uses this gradient to execute the gradient based query.\nUpdatePluginGradientWrapper can be replaced by any custom method that assigns the gradient to the data['_score_grad'] attribute of queries. Note that the gradient is expected to point in the direction of increasing score\n\nfrom emb_opt.data_source import DataPluginGradWrapper\nfrom emb_opt.update import UpdatePluginGradientWrapper, TopKContinuousUpdate\n\nHere we set up a run with the gradient based query. Since we are using gradients in the query step, we will swap out the RL update (also gradient based) with top k selection\n\nupdate_plugin = TopKContinuousUpdate(3) # update averages top 3 embeddings\nupdate_plugin_grad = UpdatePluginGradientWrapper(update_plugin, distance_penalty=0., max_norm=None)\n\n\nlrs = np.array([0.1, 0.5, 1, 5])\ndata_plugin_grad = DataPluginGradWrapper(data_plugin, lrs)\n\n\nrunner = Runner(data_plugin_grad, filter_plugin, score_plugin, None, update_plugin_grad)\n\n\ninput_batch = get_input_batch(n_queries=5)\n\n\noutput_batch, search_log = runner.search(input_batch, 5)\n\n0 0.97 0.97 0.97 0.97 0.98\n1 0.97 0.97 0.98 0.97 0.98\n2 0.98 0.97 0.97 0.98 0.98\n3 0.97 0.98 0.98 0.98 0.98\n4 0.97 0.98 0.97 0.98 0.97\n\n\n\nresults = search_log.compile_results(skip_removed=False)\nlen(results)\n\n349\n\n\n\nk_vals = [1, 5, 10, 50]\nrecovered_ids = set([i['id'] for i in results])\nmetrics = []\n\nfor k in k_vals:\n    gt_idxs = set(ground_truth[:k]['index'])\n    percent_recovered = len(gt_idxs.intersection(recovered_ids))/k\n    metrics.append(percent_recovered)\n    \nmetrics\n\n[1.0, 0.4, 0.4, 0.34]",
    "crumbs": [
      "tutorials",
      "Advanced Tutorial"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "emb_opt",
    "section": "",
    "text": "emb_opt uses reinforcement learning and hill climbing algorithms to efficiently find high scoring items in embedding spaces, such as vector databases or generative model latent spaces.\nSee the documentation site for documentation and tutorials",
    "crumbs": [
      "emb_opt"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "emb_opt",
    "section": "Install",
    "text": "Install\npip install emb_opt",
    "crumbs": [
      "emb_opt"
    ]
  },
  {
    "objectID": "index.html#supported-backends",
    "href": "index.html#supported-backends",
    "title": "emb_opt",
    "section": "Supported Backends",
    "text": "Supported Backends\nemb_opt currently supports Faiss, HuggingFace and Qdrant backends",
    "crumbs": [
      "emb_opt"
    ]
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "Data Source",
    "section": "",
    "text": "The Data Source step runs a set of queries against some data source.\nThe query is defined by the DataSourceFunction schema, which maps inputs List[Query] to outputs List[DataSourceResponse].\nThe DataSourceModule manages execution of a DataSourceFunction. The DataSourceModule gathers valid queries, sends them to the DataSourceFunction, and processes the results.\n\nsource\n\nDataSourceModule\n\n DataSourceModule (function:Callable[[List[emb_opt.schemas.Query]],List[em\n                   b_opt.schemas.DataSourceResponse]])\n\nModule - module base class\nGiven an input Batch, the Module: 1. gathers inputs to the function 2. executes the function 3. validates the results of the function with output_schema 4. scatters results back into the Batch\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunction\ntyping.Callable[[typing.List[emb_opt.schemas.Query]], typing.List[emb_opt.schemas.DataSourceResponse]]\ndata function\n\n\n\n\ndef build_batch():\n    embeddings = [[0.1], [0.2], [0.3]]\n    batch = build_batch_from_embeddings(embeddings)\n    return batch\n\ndef data_source_test(queries: List[Query]) -&gt; List[DataSourceResponse]:\n    results = []\n    for i, query in enumerate(queries):\n        if i==0:\n            response = DataSourceResponse(valid=False, data={'test':'test false response'},\n                                         query_results=[Item.from_minimal(item='', embedding=[0.1])])\n        elif i==1:\n            response = DataSourceResponse(valid=True, data={'test':'test empty response'},\n                                         query_results=[])\n        elif i==2:\n            response = DataSourceResponse(valid=True, data={'test':'test normal response'},\n                                         query_results=[Item.from_minimal(item='1', embedding=[0.1]), \n                                                       Item.from_minimal(item='2', embedding=[0.2])])\n        results.append(response)\n    return results\n\nbatch = build_batch()\ndata_module = DataSourceModule(data_source_test)\nbatch2 = data_module(batch)\nassert [i.internal.removed for i in batch2] == [True, True, False]\n\nfor q in batch2:\n    for r in q:\n        assert r.internal.parent_id == q.id\n\nGiven pydantic data parsing, the function can also return a json response\n\ndef json_test(queries: List[Query]) -&gt; List[Dict]:\n    results = [\n        {\n            'valid' : True,\n            'data' : {},\n            'query_results' : [\n                {\n                    'id' : None,\n                    'item' : 'test',\n                    'embedding' : [0.1],\n                    'score' : None,\n                    'data' : None\n                }\n            ]\n        }\n        for i in queries\n    ]\n    return results\n\nbatch = build_batch()\ndata_module = DataSourceModule(json_test)\nbatch2 = data_module(batch)\n\n\nsource\n\n\nDataSourcePlugin\n\n DataSourcePlugin ()\n\nDataSourcePlugin - documentation for plugin functions to DataSourceFunction\nA valid DataSourceFunction is any function that maps List[Query] to List[DataSourceResponse]. The inputs will be given as Query objects. The outputs can be either a list of DataSourceResponse objects or a list of valid json dictionaries that match the DataSourceResponse schema\nQuery schema:\n{     'item' : Optional[Any],     'embedding' : List[float],     'data' : Optional[Dict],     'query_results': [] # will be empty at this stage }\nItem schema:\n{     'id' : Optional[Union[str, int]]     'item' : Optional[Any],     'embedding' : List[float],     'score' : None, # will be None at this stage     'data' : Optional[Dict], }\nInput schema:\nList[Query]\nDataSourceResponse schema:\n{     'valid' : bool,     'data' : Optional[Dict],     'query_results' : List[Item] }\nOutput schema:\nList[DataSourceResponse]\nThe NumpyDataPlugin data source works with any numpy array of embeddings\n\nsource\n\n\nNumpyDataPlugin\n\n NumpyDataPlugin (k:int, item_embeddings:numpy.ndarray,\n                  item_data:Optional[List[Dict]]=None,\n                  id_key:Optional[str]=None, item_key:Optional[str]=None,\n                  distance_metric:str='euclidean',\n                  distance_cutoff:Optional[float]=None)\n\nNumpyDataPlugin - data plugin for working with numpy arrays. The data query will run k nearest neighbors of the query embeddings against the item_embeddings using distance_metric\nOptionally, item_data can be provided as a list of dicts, where item_data[i] corresponds to the data for item_embeddings[i].\nIf item_data is provided, item_data[i]['id_key'] defines the ID of item i, and item_data[i]['item_key'] defines the specific item i\ndistance_metric is any valid scipy distance metric. see https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html\nif distance_cutoff is specified, query results with a distance greater than distance_cutoff are ignored\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk\nint\n\nk nearest neighbors to return\n\n\nitem_embeddings\nndarray\n\nitem embeddings\n\n\nitem_data\ntyping.Optional[typing.List[typing.Dict]]\nNone\nOptional dict of item data\n\n\nid_key\ntyping.Optional[str]\nNone\nOptional key for item id (should be in item_data dict)\n\n\nitem_key\ntyping.Optional[str]\nNone\nOptional key for item value (should be in item_data dict)\n\n\ndistance_metric\nstr\neuclidean\ndistance metric, see options at https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html\n\n\ndistance_cutoff\ntyping.Optional[float]\nNone\nquery to result distance cutoff\n\n\n\n\nn_vectors = 256\nd_vectors = 64\nk = 10\nn_queries = 5\n\nvectors = np.random.randn(n_vectors, d_vectors)\nvector_data = [{'index':str(np.random.randint(0,1e6)), \n                'other':np.random.randint(0,1e3), \n                'item':str(np.random.randint(0,1e4))} \n               for i in range(vectors.shape[0])]\n\ndata_function = NumpyDataPlugin(k, vectors, vector_data, id_key='index', item_key='item', \n                                distance_metric='cosine', distance_cutoff=0.7)\ndata_module = DataSourceModule(data_function)\n\nbatch = build_batch_from_embeddings(np.random.randn(n_queries, d_vectors))\nbatch2 = data_module(batch)\n\nfor q in batch2:\n    for r in q:\n        assert r.internal.parent_id == q.id\n        \nassert all([max(i.data['query_distance'])&lt;0.7 for i in batch2 if i.data['query_distance']])\n\n\nsource\n\n\nDataPluginGradWrapper\n\n DataPluginGradWrapper (function:Callable[[List[emb_opt.schemas.Query]],Li\n                        st[emb_opt.schemas.DataSourceResponse]],\n                        lrs:numpy.ndarray)\n\nDataPluginGradWrapper - wraps a DataSourceFunction to allow for gradient-based queries. The score gradient is used to generate hypothetical query embeddings following new_query = old_query + lr*grad\nThis should be used in conjunction with UpdatePluginGradientWrapper or an UpdateFunction that assigns the gradient to query.data['_score_grad']\nNote that the gradient in this case is expected to point in the direction of increasing score. If using a custom gradient computation method, you may need to flip the sign of the gradient\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunction\ntyping.Callable[[typing.List[emb_opt.schemas.Query]], typing.List[emb_opt.schemas.DataSourceResponse]]\ndata function to wrap\n\n\nlrs\nndarray\narray of learning rates",
    "crumbs": [
      "Data Source"
    ]
  },
  {
    "objectID": "log.html",
    "href": "log.html",
    "title": "Log",
    "section": "",
    "text": "With branching updates, it can be hard to track the movement of an initial query to a final query. The QueryTree class constructs a tree of Node objects to map parent/child relationships between search iterations\n\nsource\n\nNode\n\n Node (query:emb_opt.schemas.Query, iteration:int)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nQueryTree\n\n QueryTree ()\n\nInitialize self. See help(type(self)) for accurate signature.\nSearch iteration batches are logged in the Log class\n\nsource\n\n\nLog\n\n Log ()\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "Log"
    ]
  },
  {
    "objectID": "schemas.html",
    "href": "schemas.html",
    "title": "Schemas",
    "section": "",
    "text": "emb_opt is designed to run hill climbing algorithms in embedding spaces. In practice, this means we are searching through some explicit vector database or the implicit embedding space of some generative model, which we refer to as a DataSource. We denote the continuous space as referring to embeddings, and the discrete space as referring to discrete things represented by embeddings.\nThe DataSource is queried with a Query. The Query contains a query embedding and optionally an item (some discrete thing represented by the embedding). The DataSource uses the Query to return a list of Item objects. An Item represents a discrete thing returned by the DataSource\nThe Item results are optionally sent to a Filter, which removes results based on some True/False criteria.\nThe Item results are then sent to a Score which assigns some numeric score value to each Item.\nThe Query and scored Item results are sent to a Update which uses the scored items to generate a new Query. Update methods are denoted as discrete or continuous. continuous updates generate new queries purely in embedding space (ie by averaging Item embeddings). discrete updates create new queries specifically from Item results, such that each query can have a specific item associated with it (not possible with continuous updates). continuous updates generally converge faster, but certain types of DataSource may require a discrete item query and therefore be incompatible with continuous updates.\nSome Update methods generate multiple new queries. To control the total number of queries, a Prune step is optionally added before the Update step.\nThe general flow is: 1. Start with a Batch of Query objects * Query the DataSource * (optional) Send results to the Filter * Send results to the Score * (optional) Prune queries * Use scored results to Update to a new set of queries\nThe schemas present here define the required input/output structure for each step to allow for fully flexible plugins to the process\n\n\n\n\nInternalData tracks internal information as part of the embedding search. This data is managed internally, but may be useful for certain Prune or Update configurations.\nInternalData.removed denotes if the related Item or Query has been removed or invalidated by some step (see DataSourceResponse, FilterResponse, ScoreResponse, PruneResponse)\nInternalData.removal_reason details the removal reason\nInternalData.parent_id is the ID string of the parent Query to the related Item or Query object. InternalData.parent_id always points to a Query, never an Item\nInternalData.collection_id groups Item and Query objects that come from the same initial Query. This is useful when an Update step generates multiple new queries from a single input\nInternalData.iteration denotes which iteration of the search created the related Item or Query\n\nsource\n\n\n\n\n\n InteralData (removed:bool, removal_reason:Optional[str],\n              parent_id:Optional[str], collection_id:Optional[int],\n              iteration:Optional[int])\n\nInternal Data Tracking\n\n\nThe Item schema is the basic “object” or “thing” we are looking for. The goal of emb_opt is to discover an Item with a high score\nItem.id is the index/ID of the item (for example the database index). If no ID is provided, one will be created as a UUID. emb_opt assumes Item.id is unique to the item.\nItem.item is the discrete thing itself\nItem.score is the score of the item. emb_opt assumes a hill climbing scenario where higher scores are better than lower scores.\nItem.data is a dictionary container for any other information associated with the item (ie other fields returned from a database query)\n\nsource\n\n\n\n\n\n Item (id:Union[int,str,NoneType], item:Optional[Any],\n       embedding:List[float], score:Optional[float], data:Optional[dict],\n       **extra_data:Any)\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\nitem = Item(id=None, embedding=[0.1], item=None, score=None, data=None)\nassert item.id\nold_id = item.id\nitem = Item.model_validate(item)\nassert item.id == old_id\n\n\n\nA Query is the basic object for searching a DataSource and holding Item results returned by the search.\nQuery.item is an (optional) discrete item associated with the Query. This is populated automatically when they query is created from an Item via Query.from_item\nQuery.embedding is the embedding associated with the Query\nQuery.data is a dictionary container for any other information associated with the query\nQuery.query_results is a list of Item objects returned from a query\n\nsource\n\n\n\n\n\n Query (item:Optional[Any], embedding:List[float], data:Optional[dict],\n        query_results:Optional[list[__main__.Item]], **extra_data:Any)\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\nA Query holds Items, tracks parent/child relationships, and allows for convenient iteration\n\nquery = Query.from_minimal(embedding=[0.1])\nquery.update_internal(collection_id=0) # add collection ID\n\nquery_results = [\n    Item.from_minimal(item='item1', embedding=[0.1]),\n    Item.from_minimal(item='item2', embedding=[0.1]),\n]\n\nquery.add_query_results(query_results)\n\n# iteration over query results\nassert len([i for i in query]) == 2\n\n# propagation of query parent data\n\nfor query_result in query:\n    assert query_result.internal.parent_id == query.id\n    assert query_result.internal.collection_id == query.internal.collection_id\n\nItems may be removed by various steps. Removed items are kept within the Query for logging purposes. Query.valid_results and Query.enumerate_query_results allow us to automatically skip removed items during iteration\n\nassert len(list(query.valid_results())) == 2\n\nquery.query_results[0].update_internal(removed=True) # set first result to removed\n\nassert len(list(query.valid_results())) == 1\n\nassert len(list(query.enumerate_query_results())) == 1\nassert len(list(query.enumerate_query_results(skip_removed=False))) == 2\n\nquery.query_results[1].update_internal(removed=True) # set second result to removed\nquery.update_internal() # update query internal\nassert query.internal.removed # query sets itself to removed when all query results are removed\n\nQueries can be created from another Query or another Item, with automatic data propagation between them\n\n# create query from item\nitem = Item.from_minimal(item='test_item', embedding=[0.1])\nquery = Query.from_item(item)\nassert query.item == item.item\n\n# create query from query\nquery = Query.from_minimal(embedding=[0.1])\nnew_query = Query.from_parent_query(embedding=[0.2], parent_query=query)\nassert new_query.internal.parent_id == query.id\n\n\n\nThe Batch object holds a list of Query objects and provides convenience functions for iterating over queries and query results\n\nsource\n\n\n\n\n\n Batch (queries:List[__main__.Query])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\nA Batch allows us to iterate over the queries and items in the batch in several ways\n\ndef build_test_batch(n_queries, n_items):\n    queries = []\n    \n    for i in range(n_queries):\n        query = Query.from_minimal(item=f'query_{i}', embedding=[0.1])\n        for j in range(n_items):\n            item = Item.from_minimal(item=f'item_{j}', embedding=[0.1])\n            query.add_query_results([item])\n        queries.append(query)\n    return Batch(queries=queries)\n\nn_queries = 3\nn_items = 4\nbatch = build_test_batch(n_queries, n_items)\n\nassert len(list(batch.valid_queries())) == n_queries\n\nidxs, results = batch.flatten_query_results()\nassert len(results) == n_queries*n_items\nassert batch.get_item(*idxs[0]) == batch[idxs[0][0]][idxs[0][1]]\n\nWhen items or queries are removed, this is accounted for\n\nbatch = build_test_batch(n_queries, n_items)\n\nbatch[1].update_internal(removed=True) # invalidate query\nbatch[0][0].update_internal(removed=True) # invalidate item\nbatch[0][1].update_internal(removed=True) # invalidate item\n\nassert len(list(batch.valid_queries())) == n_queries-1 # 1 batch removed\n\nidxs, results = batch.flatten_query_results(skip_removed=False) # return all queries\nassert len(results) == n_queries*n_items\n\n# skips results where `removed=True`, and all results under a query with `removed=True`\nidxs, results = batch.flatten_query_results(skip_removed=True)\n\n# n_items removed from invalid query 1, 2 items invalidated\nassert len(results) == n_queries*n_items - n_items - 2\n\n\n\n\nThe DataSourceFunction schema defines the interface for data source queries. The function takes a list of MinimalQuery objects and returns a list of DataSourceResponse objects.\n\nsource\n\n\n\n\n DataSourceResponse (valid:bool, data:Optional[Dict],\n                     query_results:List[__main__.Item])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\n\n\nThe FilterFunction schema defines the interface for filtering result items. The function takes a list of Item objects and returns a list of FilterResponse objects.\n\nsource\n\n\n\n\n FilterResponse (valid:bool, data:Optional[Dict])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\n\n\nThe ScoreFunction schema defines the interface for scoring result items. The function takes a list of Item objects and returns a list of ScoreResponse objects.\n\nsource\n\n\n\n\n ScoreResponse (valid:bool, score:Optional[float], data:Optional[Dict])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\n\n\nThe PruneFunction schema defines the interface for pruning queries. The function takes a list of Query objects and returns a list of PruneResponse objects.\n\nsource\n\n\n\n\n PruneResponse (valid:bool, data:Optional[Dict])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\n\n\nThe UpdateFunction schema defines the interface for pruning queries. The function takes a list of Query objects and returns a list of new Query objects.\n\nsource\n\n\n\n\n UpdateResponse (query:__main__.Query, parent_id:Optional[str])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.",
    "crumbs": [
      "Schemas"
    ]
  },
  {
    "objectID": "schemas.html#high-level-overview",
    "href": "schemas.html#high-level-overview",
    "title": "Schemas",
    "section": "",
    "text": "emb_opt is designed to run hill climbing algorithms in embedding spaces. In practice, this means we are searching through some explicit vector database or the implicit embedding space of some generative model, which we refer to as a DataSource. We denote the continuous space as referring to embeddings, and the discrete space as referring to discrete things represented by embeddings.\nThe DataSource is queried with a Query. The Query contains a query embedding and optionally an item (some discrete thing represented by the embedding). The DataSource uses the Query to return a list of Item objects. An Item represents a discrete thing returned by the DataSource\nThe Item results are optionally sent to a Filter, which removes results based on some True/False criteria.\nThe Item results are then sent to a Score which assigns some numeric score value to each Item.\nThe Query and scored Item results are sent to a Update which uses the scored items to generate a new Query. Update methods are denoted as discrete or continuous. continuous updates generate new queries purely in embedding space (ie by averaging Item embeddings). discrete updates create new queries specifically from Item results, such that each query can have a specific item associated with it (not possible with continuous updates). continuous updates generally converge faster, but certain types of DataSource may require a discrete item query and therefore be incompatible with continuous updates.\nSome Update methods generate multiple new queries. To control the total number of queries, a Prune step is optionally added before the Update step.\nThe general flow is: 1. Start with a Batch of Query objects * Query the DataSource * (optional) Send results to the Filter * Send results to the Score * (optional) Prune queries * Use scored results to Update to a new set of queries\nThe schemas present here define the required input/output structure for each step to allow for fully flexible plugins to the process\n\n\n\n\nInternalData tracks internal information as part of the embedding search. This data is managed internally, but may be useful for certain Prune or Update configurations.\nInternalData.removed denotes if the related Item or Query has been removed or invalidated by some step (see DataSourceResponse, FilterResponse, ScoreResponse, PruneResponse)\nInternalData.removal_reason details the removal reason\nInternalData.parent_id is the ID string of the parent Query to the related Item or Query object. InternalData.parent_id always points to a Query, never an Item\nInternalData.collection_id groups Item and Query objects that come from the same initial Query. This is useful when an Update step generates multiple new queries from a single input\nInternalData.iteration denotes which iteration of the search created the related Item or Query\n\nsource\n\n\n\n\n\n InteralData (removed:bool, removal_reason:Optional[str],\n              parent_id:Optional[str], collection_id:Optional[int],\n              iteration:Optional[int])\n\nInternal Data Tracking\n\n\nThe Item schema is the basic “object” or “thing” we are looking for. The goal of emb_opt is to discover an Item with a high score\nItem.id is the index/ID of the item (for example the database index). If no ID is provided, one will be created as a UUID. emb_opt assumes Item.id is unique to the item.\nItem.item is the discrete thing itself\nItem.score is the score of the item. emb_opt assumes a hill climbing scenario where higher scores are better than lower scores.\nItem.data is a dictionary container for any other information associated with the item (ie other fields returned from a database query)\n\nsource\n\n\n\n\n\n Item (id:Union[int,str,NoneType], item:Optional[Any],\n       embedding:List[float], score:Optional[float], data:Optional[dict],\n       **extra_data:Any)\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\nitem = Item(id=None, embedding=[0.1], item=None, score=None, data=None)\nassert item.id\nold_id = item.id\nitem = Item.model_validate(item)\nassert item.id == old_id\n\n\n\nA Query is the basic object for searching a DataSource and holding Item results returned by the search.\nQuery.item is an (optional) discrete item associated with the Query. This is populated automatically when they query is created from an Item via Query.from_item\nQuery.embedding is the embedding associated with the Query\nQuery.data is a dictionary container for any other information associated with the query\nQuery.query_results is a list of Item objects returned from a query\n\nsource\n\n\n\n\n\n Query (item:Optional[Any], embedding:List[float], data:Optional[dict],\n        query_results:Optional[list[__main__.Item]], **extra_data:Any)\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\nA Query holds Items, tracks parent/child relationships, and allows for convenient iteration\n\nquery = Query.from_minimal(embedding=[0.1])\nquery.update_internal(collection_id=0) # add collection ID\n\nquery_results = [\n    Item.from_minimal(item='item1', embedding=[0.1]),\n    Item.from_minimal(item='item2', embedding=[0.1]),\n]\n\nquery.add_query_results(query_results)\n\n# iteration over query results\nassert len([i for i in query]) == 2\n\n# propagation of query parent data\n\nfor query_result in query:\n    assert query_result.internal.parent_id == query.id\n    assert query_result.internal.collection_id == query.internal.collection_id\n\nItems may be removed by various steps. Removed items are kept within the Query for logging purposes. Query.valid_results and Query.enumerate_query_results allow us to automatically skip removed items during iteration\n\nassert len(list(query.valid_results())) == 2\n\nquery.query_results[0].update_internal(removed=True) # set first result to removed\n\nassert len(list(query.valid_results())) == 1\n\nassert len(list(query.enumerate_query_results())) == 1\nassert len(list(query.enumerate_query_results(skip_removed=False))) == 2\n\nquery.query_results[1].update_internal(removed=True) # set second result to removed\nquery.update_internal() # update query internal\nassert query.internal.removed # query sets itself to removed when all query results are removed\n\nQueries can be created from another Query or another Item, with automatic data propagation between them\n\n# create query from item\nitem = Item.from_minimal(item='test_item', embedding=[0.1])\nquery = Query.from_item(item)\nassert query.item == item.item\n\n# create query from query\nquery = Query.from_minimal(embedding=[0.1])\nnew_query = Query.from_parent_query(embedding=[0.2], parent_query=query)\nassert new_query.internal.parent_id == query.id\n\n\n\nThe Batch object holds a list of Query objects and provides convenience functions for iterating over queries and query results\n\nsource\n\n\n\n\n\n Batch (queries:List[__main__.Query])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\nA Batch allows us to iterate over the queries and items in the batch in several ways\n\ndef build_test_batch(n_queries, n_items):\n    queries = []\n    \n    for i in range(n_queries):\n        query = Query.from_minimal(item=f'query_{i}', embedding=[0.1])\n        for j in range(n_items):\n            item = Item.from_minimal(item=f'item_{j}', embedding=[0.1])\n            query.add_query_results([item])\n        queries.append(query)\n    return Batch(queries=queries)\n\nn_queries = 3\nn_items = 4\nbatch = build_test_batch(n_queries, n_items)\n\nassert len(list(batch.valid_queries())) == n_queries\n\nidxs, results = batch.flatten_query_results()\nassert len(results) == n_queries*n_items\nassert batch.get_item(*idxs[0]) == batch[idxs[0][0]][idxs[0][1]]\n\nWhen items or queries are removed, this is accounted for\n\nbatch = build_test_batch(n_queries, n_items)\n\nbatch[1].update_internal(removed=True) # invalidate query\nbatch[0][0].update_internal(removed=True) # invalidate item\nbatch[0][1].update_internal(removed=True) # invalidate item\n\nassert len(list(batch.valid_queries())) == n_queries-1 # 1 batch removed\n\nidxs, results = batch.flatten_query_results(skip_removed=False) # return all queries\nassert len(results) == n_queries*n_items\n\n# skips results where `removed=True`, and all results under a query with `removed=True`\nidxs, results = batch.flatten_query_results(skip_removed=True)\n\n# n_items removed from invalid query 1, 2 items invalidated\nassert len(results) == n_queries*n_items - n_items - 2\n\n\n\n\nThe DataSourceFunction schema defines the interface for data source queries. The function takes a list of MinimalQuery objects and returns a list of DataSourceResponse objects.\n\nsource\n\n\n\n\n DataSourceResponse (valid:bool, data:Optional[Dict],\n                     query_results:List[__main__.Item])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\n\n\nThe FilterFunction schema defines the interface for filtering result items. The function takes a list of Item objects and returns a list of FilterResponse objects.\n\nsource\n\n\n\n\n FilterResponse (valid:bool, data:Optional[Dict])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\n\n\nThe ScoreFunction schema defines the interface for scoring result items. The function takes a list of Item objects and returns a list of ScoreResponse objects.\n\nsource\n\n\n\n\n ScoreResponse (valid:bool, score:Optional[float], data:Optional[Dict])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\n\n\nThe PruneFunction schema defines the interface for pruning queries. The function takes a list of Query objects and returns a list of PruneResponse objects.\n\nsource\n\n\n\n\n PruneResponse (valid:bool, data:Optional[Dict])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\n\n\nThe UpdateFunction schema defines the interface for pruning queries. The function takes a list of Query objects and returns a list of new Query objects.\n\nsource\n\n\n\n\n UpdateResponse (query:__main__.Query, parent_id:Optional[str])\n\nUsage docs: https://docs.pydantic.dev/2.7/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.",
    "crumbs": [
      "Schemas"
    ]
  },
  {
    "objectID": "plugins/qdrant.html",
    "href": "plugins/qdrant.html",
    "title": "Qdrant Plugins",
    "section": "",
    "text": "Qdrant Data Plugin\nQdrantDataPlugin integrates with a Qdrant database. search_request_kwargs can be any valid inputs to a Qdrant SearchRequest\n\nsource\n\n\nQdrantDataPlugin\n\n QdrantDataPlugin (k:int, collection_name:str,\n                   qdrant_client:qdrant_client.qdrant_client.QdrantClient,\n                   item_key:Optional[str]=None,\n                   search_request_kwargs:Optional[dict]=None,\n                   distance_cutoff:Optional[float]=None)\n\nQdrantDataPlugin - data plugin for working with a qdrant vector database.\nThe data query will run k nearest neighbors against the qdrant collection collection_name\nOptionally, item_key denotes the key in an object’s payload corresponding to the item value\nsearch_request_kwargs are optional kwargs sent to models.SearchRequest\nif distance_cutoff is specified, query results with a distance greater than distance_cutoff are ignored\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk\nint\n\nk nearest neighbors to return\n\n\ncollection_name\nstr\n\nqdrant collection name\n\n\nqdrant_client\nQdrantClient\n\nqdrant client\n\n\nitem_key\ntyping.Optional[str]\nNone\nkey in qdrant payload denoting item value\n\n\nsearch_request_kwargs\ntyping.Optional[dict]\nNone\noptional kwargs for SearchRequest\n\n\ndistance_cutoff\ntyping.Optional[float]\nNone\nquery to result distance cutoff\n\n\n\n\nn_vectors = 1000\nd_vectors = 128\nn_queries = 5\n\nvectors = np.random.randn(n_vectors, d_vectors)\npayloads = [{'rand':np.random.rand(), 'item' : str(np.random.randint(0, 1e6))} for i in range(n_vectors)]\nqueries = np.random.randn(n_queries, d_vectors)\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\nclient.recreate_collection(\n    collection_name=\"test_collection\",\n    vectors_config=models.VectorParams(size=d_vectors, distance=models.Distance.EUCLID),\n)\n\noperation_info = client.upsert(\n    collection_name=\"test_collection\",\n    points=models.Batch(\n        ids=[i for i in range(n_vectors)],\n        payloads=payloads,\n        vectors=vectors.tolist()\n    )\n)\n\nclient.update_collection(\n            collection_name='test_collection',\n            optimizer_config=models.OptimizersConfigDiff(\n                indexing_threshold=1\n            )\n        )\n\nsearch_filter = models.Filter(\n    must=[\n            models.FieldCondition(\n            key=\"rand\",\n            range=models.Range(\n                gt=None,\n                gte=None,\n                lt=None,\n                lte=0.8,\n            ),\n        )\n    ]\n)\n\ndata_function = QdrantDataPlugin(5, \"test_collection\", client, item_key='item', \n                             search_request_kwargs={'filter' : search_filter})\n\ndata_module = DataSourceModule(data_function)\n\nbatch = build_batch_from_embeddings(queries)\nbatch2 = data_module(batch)\n\nfor q in batch2:\n    for r in q:\n        assert r.internal.parent_id == q.id",
    "crumbs": [
      "plugins",
      "Qdrant Plugins"
    ]
  },
  {
    "objectID": "plugins/hf.html",
    "href": "plugins/hf.html",
    "title": "Huggingface Plugins",
    "section": "",
    "text": "Dataset Executor\nDatasetExecutor is an Executor that uses the Datasets library as a backend for parallel processing\n\nsource\n\n\nDatasetExecutor\n\n DatasetExecutor (function:Callable, batched:bool, batch_size:int=1,\n                  concurrency:Optional[int]=1,\n                  map_kwargs:Optional[dict]=None)\n\nDatasetExecutor - executes function in parallel using Dataset.map\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunction\ntyping.Callable\n\nfunction to be wrapped\n\n\nbatched\nbool\n\nif inputs should be batched\n\n\nbatch_size\nint\n1\nbatch size (set batch_size=0 to pass all inputs)\n\n\nconcurrency\ntyping.Optional[int]\n1\nnumber of concurrent threads\n\n\nmap_kwargs\ntyping.Optional[dict]\nNone\nkwargs for Dataset.map\n\n\n\n\nclass TestInput(BaseModel):\n    value: float\n        \nclass TestOutput(BaseModel):\n    result: bool\n\ndef test_function_hf(input: dict) -&gt; dict:\n    return {'result' : input['value']&gt;0.5}\n\ndef test_function_hf_batched(input: dict) -&gt; dict:\n    return {'result' : [i&gt;0.5 for i in input['value']]}\n        \nnp.random.seed(42)\nvalues = np.random.uniform(size=100).tolist()\ninputs = [TestInput(value=i) for i in values]\nexpected_outputs = [TestOutput(result=i&gt;0.5) for i in values]\n\n# dataset\n\nexecutor = DatasetExecutor(test_function_hf, batched=False, concurrency=None, batch_size=1)\nres11 = executor(inputs)\nassert [TestOutput.model_validate(i) for i in res11] == expected_outputs\n\nexecutor = DatasetExecutor(test_function_hf, batched=False, concurrency=2, batch_size=1)\nres12 = executor(inputs)\nassert [TestOutput.model_validate(i) for i in res12] == expected_outputs\n\nexecutor = DatasetExecutor(test_function_hf_batched, batched=True, concurrency=2, batch_size=5)\nres13 = executor(inputs)\nassert [TestOutput.model_validate(i) for i in res13] == expected_outputs\n\nexecutor = DatasetExecutor(test_function_hf_batched, batched=True, concurrency=None, batch_size=5)\nres14 = executor(inputs)\nassert [TestOutput.model_validate(i) for i in res14] == expected_outputs\n\n                                                                                \n\n\n\n\nData Plugin\nThe HugggingfaceDataPlugin uses a Huggingface Dataset with a faiss embedding index as a data source\n\nsource\n\n\nHugggingfaceDataPlugin\n\n HugggingfaceDataPlugin (k:int, dataset:datasets.arrow_dataset.Dataset,\n                         index_name:str, item_key:Optional[str]=None,\n                         id_key:Optional[str]=None,\n                         distance_cutoff:Optional[float]=None)\n\nHugggingfaceDataPlugin - data plugin for working with huggingface datasets library.\nThe input dataset should have a faiss embedding index denoted by index_name.\nThe data query will run k nearest neighbors against the dataset index based on the metric used to create the index\nOptionally, item_key denotes the column in dataset defining a specific item, and id_key denotes the column defining an item’s ID\nif distance_cutoff is specified, query results with a distance greater than distance_cutoff are ignored\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk\nint\n\nk nearest neighbors to return\n\n\ndataset\nDataset\n\ninput dataset\n\n\nindex_name\nstr\n\nname of the faiss index in dataset\n\n\nitem_key\ntyping.Optional[str]\nNone\ndataset column denoting item value\n\n\nid_key\ntyping.Optional[str]\nNone\ndataset column denoting item id\n\n\ndistance_cutoff\ntyping.Optional[float]\nNone\nquery to result distance cutoff\n\n\n\n\nn_vectors = 256\nd_vectors = 64\nk = 10\nn_queries = 5\n\nvectors = np.random.randn(n_vectors, d_vectors)\n\nvector_data = [{'index':str(np.random.randint(0,1e6)), \n                'other':np.random.randint(0,1e3), \n                'item':str(np.random.randint(0,1e4)),\n                'embedding':vectors[i]\n               } \n               for i in range(vectors.shape[0])]\n\ndataset = Dataset.from_list(vector_data)\ndataset.add_faiss_index('embedding')\n\ndata_function = HugggingfaceDataPlugin(k, dataset, 'embedding', 'item', 'index')\ndata_module = DataSourceModule(data_function)\n\nbatch = build_batch_from_embeddings(np.random.randn(n_queries, d_vectors))\nbatch2 = data_module(batch)\n\nfor q in batch2:\n    for r in q:\n        assert r.internal.parent_id == q.id\n\n100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 1936.43it/s]",
    "crumbs": [
      "plugins",
      "Huggingface Plugins"
    ]
  },
  {
    "objectID": "module.html",
    "href": "module.html",
    "title": "Module",
    "section": "",
    "text": "source\n\nModule\n\n Module (output_schema:pydantic.main.BaseModel, function:Callable[[List[py\n         dantic.main.BaseModel]],List[pydantic.main.BaseModel]])\n\nModule - module base class\nGiven an input Batch, the Module: 1. gathers inputs to the function 2. executes the function 3. validates the results of the function with output_schema 4. scatters results back into the Batch\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\noutput_schema\nBaseModel\nexpected output schema\n\n\nfunction\ntyping.Callable[[typing.List[pydantic.main.BaseModel]], typing.List[pydantic.main.BaseModel]]\nfunction to be called",
    "crumbs": [
      "Module"
    ]
  },
  {
    "objectID": "plugins/faiss.html",
    "href": "plugins/faiss.html",
    "title": "Fauss Plugins",
    "section": "",
    "text": "Faiss Data Plugin\nThe FaissDataPlugin integrates with a faiss index.\nsearch_params can be any params object compatible with faiss search\n\nsource\n\n\nFaissDataPlugin\n\n FaissDataPlugin (k:int, faiss_index:faiss.swigfaiss_avx2.Index,\n                  item_data:Optional[List[Dict]]=None,\n                  item_key:Optional[str]=None, search_params:Optional[fais\n                  s.swigfaiss_avx2.SearchParameters]=None,\n                  distance_cutoff:Optional[float]=None)\n\nFaissDataPlugin - data plugin for working with a faiss vector index\nThe data query will run k nearest neighbors against faiss_index\nOptionally, item_data can be provided as a list of dicts, where item_data[i] corresponds to the data for embedding i in the faiss index\nIf item_data is provided item_data[i]['item_key'] defines the specific value for item i\nsearch_params are optional kwargs sent to faiss.SearchParameters\nif distance_cutoff is specified, query results with a distance greater than distance_cutoff are ignored\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk\nint\n\nk nearest neighbors to return\n\n\nfaiss_index\nIndex\n\nfaiss index\n\n\nitem_data\ntyping.Optional[typing.List[typing.Dict]]\nNone\nOptional dict of item data\n\n\nitem_key\ntyping.Optional[str]\nNone\nOptional key for item value (should be in item_data dict)\n\n\nsearch_params\ntyping.Optional[faiss.swigfaiss_avx2.SearchParameters]\nNone\nfaiss search params\n\n\ndistance_cutoff\ntyping.Optional[float]\nNone\nquery to result distance cutoff\n\n\n\n\nn_vectors = 256\nd_vectors = 64\nk = 10\nn_queries = 5\n\nvectors = np.random.randn(n_vectors, d_vectors)\n\nvector_data = [{'other':np.random.randint(0,1e3), \n                'item':str(np.random.randint(0,1e4))} \n               for i in range(vectors.shape[0])]\n\nindex = faiss.IndexFlatL2(d_vectors)\nindex.add(vectors)\n\ndata_function = FaissDataPlugin(5, index, vector_data, 'item')\ndata_module = DataSourceModule(data_function)\n\nbatch = build_batch_from_embeddings(np.random.randn(n_queries, d_vectors))\nbatch2 = data_module(batch)",
    "crumbs": [
      "plugins",
      "Fauss Plugins"
    ]
  },
  {
    "objectID": "prune.html",
    "href": "prune.html",
    "title": "Prune",
    "section": "",
    "text": "The Prune step optionally removes queries prior to the update step. A Prune step allows for control over the total number of queries in the scenario where the Update step generates multiple output queries for each input.\nThe prune step is formalized by the PruneFunction schema, which maps inputs List[Query] to outputs List[PruneResponse].\nThe PruneModule manages execution of a PruneFunction. The PruneModule gathers valid items, sends them to the PruneFunction, and processes the results.\n\nsource\n\nPruneModule\n\n PruneModule (function:Callable[[List[emb_opt.schemas.Query]],List[emb_opt\n              .schemas.PruneResponse]])\n\nModule - module base class\nGiven an input Batch, the Module: 1. gathers inputs to the function 2. executes the function 3. validates the results of the function with output_schema 4. scatters results back into the Batch\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunction\ntyping.Callable[[typing.List[emb_opt.schemas.Query]], typing.List[emb_opt.schemas.PruneResponse]]\nprune function\n\n\n\n\nbatch = Batch(queries=[\n                        Query.from_minimal(embedding=[0.1]),\n                        Query.from_minimal(embedding=[0.2]),\n                        Query.from_minimal(embedding=[0.3]),\n                    ])\n\ndef prune_func(queries):\n    return [PruneResponse(valid=i.embedding[0]&gt;=0.2, data=None) for i in queries]\n\nprune_module = PruneModule(prune_func)\n\nbatch = prune_module(batch)\n\nassert [i.internal.removed for i in batch] == [True, False, False]\n\n\nsource\n\n\nPrunePlugin\n\n PrunePlugin ()\n\nPrunePlugin - documentation for plugin functions to PruneFunction\nA valid PruneFunction is any function that maps List[Query] to List[PruneResponse]. The inputs will be given as Query objects. The outputs can be either a list of PruneResponse objects or a list of valid json dictionaries that match the PruneResponse schema\nThe Prune step is called after scoring, so each result Item in the input queries will have a score assigned\nItem schema:\n{     'id' : Optional[Union[str, int]]     'item' : Optional[Any],     'embedding' : List[float],     'score' : float,     'data' : Optional[Dict], }\nQuery schema:\n{     'item' : Optional[Any],     'embedding' : List[float],     'data' : Optional[Dict],     'query_results': List[Item] }\nInput schema:\nList[Query]\nPruneResponse schema:\n{     'valid' : bool,     'data' : Optional[Dict], }\nOutput schema:\nList[PruneResponse]\n\nsource\n\n\nTopKPrune\n\n TopKPrune (k:int, score_agg:str='mean',\n            group_by:Optional[str]='collection_id')\n\nTopKPrune - keeps the top k best queries in each group by aggregated score\nqueries are first grouped by group_by * if group_by=None, all queries are considered the same group (global pruning) * if group_by='parent_id', queries are grouped by parent query id * if `group_by=‘collection_id’, queries are grouped by collection id\nqueries are then assigned a score based on aggregating query result scores * if score_agg='mean', each Query is scored by the average score of all Item results * if score_agg='max', each Query is scored by the max scoring Item result\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk\nint\n\n\n\n\nscore_agg\nstr\nmean\n[‘mean’, ‘max’]\n\n\ngroup_by\ntyping.Optional[str]\ncollection_id\n[None, ‘collection_id’, ‘parent_id’]\n\n\n\n\nq1 = Query.from_minimal(embedding=[0.1])\nq1.update_internal(collection_id=0)\nq1.add_query_results([\n    Item.from_minimal(embedding=[0.11], score=-10),\n    Item.from_minimal(embedding=[0.12], score=6),\n])\n\nq2 = Query.from_minimal(embedding=[0.2])\nq2.update_internal(collection_id=0)\nq2.add_query_results([\n    Item.from_minimal(embedding=[0.21], score=4),\n    Item.from_minimal(embedding=[0.22], score=5),\n])\n\nq3 = Query.from_minimal(embedding=[0.3])\nq3.update_internal(collection_id=1)\nq3.add_query_results([\n    Item.from_minimal(embedding=[0.31], score=7),\n    Item.from_minimal(embedding=[0.32], score=8),\n])\n\nqueries = [q1, q2, q3]\n\nprune_func = TopKPrune(k=1, score_agg='mean', group_by=None)\n\nassert [i.valid for i in prune_func(queries)] == [False, False, True]\n\n\nq1 = Query.from_minimal(embedding=[0.1])\nq1.update_internal(collection_id=0)\nq1.add_query_results([\n    Item.from_minimal(embedding=[0.11], score=-10),\n    Item.from_minimal(embedding=[0.12], score=6),\n])\n\nq2 = Query.from_minimal(embedding=[0.2])\nq2.update_internal(collection_id=0)\nq2.add_query_results([\n    Item.from_minimal(embedding=[0.21], score=4),\n    Item.from_minimal(embedding=[0.22], score=5),\n])\n\nq3 = Query.from_minimal(embedding=[0.3])\nq3.update_internal(collection_id=1)\nq3.add_query_results([\n    Item.from_minimal(embedding=[0.31], score=7),\n    Item.from_minimal(embedding=[0.32], score=8),\n])\n\nqueries = [q1, q2, q3]\n\nprune_func = TopKPrune(k=1, score_agg='max', group_by='collection_id')\n\nassert [i.valid for i in prune_func(queries)] == [True, False, True]\n\nprune_func = TopKPrune(k=1, score_agg='mean', group_by='collection_id')\n\nassert [i.valid for i in prune_func(queries)] == [False, True, True]",
    "crumbs": [
      "Prune"
    ]
  },
  {
    "objectID": "executor.html",
    "href": "executor.html",
    "title": "executor",
    "section": "",
    "text": "Executors\nExecutor classes are helper classes for batching and parallel processing\n\nsource\n\n\nExecutor\n\n Executor (function:Callable, batched:bool, batch_size:int=1)\n\nBasic Executor class. Batches inputs, sends batches to function, unbatches outputs\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunction\ntyping.Callable\n\nfunction to be wrapped\n\n\nbatched\nbool\n\nif inputs should be batched\n\n\nbatch_size\nint\n1\nbatch size (set batch_size=0 to pass all inputs)\n\n\n\n\nclass TestInput(BaseModel):\n    value: float\n        \nclass TestOutput(BaseModel):\n    result: bool\n        \ndef test_function(input: TestInput) -&gt; TestOutput:\n    return TestOutput(result=input.value&gt;0.5)\n\ndef test_function_batched(inputs: list[TestInput]) -&gt; list[TestOutput]:\n    return [TestOutput(result=i.value&gt;0.5) for i in inputs]\n        \nnp.random.seed(42)\nvalues = np.random.uniform(size=100).tolist()\ninputs = [TestInput(value=i) for i in values]\nexpected_outputs = [TestOutput(result=i&gt;0.5) for i in values]\n\n# standard\n\nexecutor = Executor(test_function, batched=False)\nres1 = executor(inputs)\nassert res1 == expected_outputs\n\nexecutor = Executor(test_function_batched, batched=True, batch_size=5)\nres2 = executor(inputs)\nassert res2 == expected_outputs\n\n\nsource\n\n\nProcessExecutor\n\n ProcessExecutor (function:Callable, batched:bool, batch_size:int=1,\n                  concurrency:Optional[int]=1)\n\nProcessExecutor - executes function with multiprocessing using ProcessPoolExecutor\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunction\ntyping.Callable\n\nfunction to be wrapped\n\n\nbatched\nbool\n\nif inputs should be batched\n\n\nbatch_size\nint\n1\nbatch size (set batch_size=0 to pass all inputs)\n\n\nconcurrency\ntyping.Optional[int]\n1\nnumber of concurrent processes\n\n\n\n\nclass TestInput(BaseModel):\n    value: float\n        \nclass TestOutput(BaseModel):\n    result: bool\n        \ndef test_function(input: TestInput) -&gt; TestOutput:\n    return TestOutput(result=input.value&gt;0.5)\n\ndef test_function_batched(inputs: list[TestInput]) -&gt; list[TestOutput]:\n    return [TestOutput(result=i.value&gt;0.5) for i in inputs]\n        \nnp.random.seed(42)\nvalues = np.random.uniform(size=100).tolist()\ninputs = [TestInput(value=i) for i in values]\nexpected_outputs = [TestOutput(result=i&gt;0.5) for i in values]\n\n# process\n\nexecutor = ProcessExecutor(test_function, batched=False, concurrency=1)\nres3 = executor(inputs)\nassert res3 == expected_outputs\n\nexecutor = ProcessExecutor(test_function, batched=False, concurrency=2)\nres4 = executor(inputs)\nassert res4 == expected_outputs\n\nexecutor = ProcessExecutor(test_function_batched, batched=True, batch_size=5)\nres5 = executor(inputs)\nassert res5 == expected_outputs\n\nexecutor = ProcessExecutor(test_function_batched, batched=True, batch_size=5, concurrency=2)\nres6 = executor(inputs)\nassert res6 == expected_outputs\n\n\nsource\n\n\nThreadExecutor\n\n ThreadExecutor (function:Callable, batched:bool, batch_size:int=1,\n                 concurrency:Optional[int]=1)\n\nProcessExecutor - executes function with multiple threads using ThreadPoolExecutor\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunction\ntyping.Callable\n\nfunction to be wrapped\n\n\nbatched\nbool\n\nif inputs should be batched\n\n\nbatch_size\nint\n1\nbatch size (set batch_size=0 to pass all inputs)\n\n\nconcurrency\ntyping.Optional[int]\n1\nnumber of concurrent threads\n\n\n\n\nclass TestInput(BaseModel):\n    value: float\n        \nclass TestOutput(BaseModel):\n    result: bool\n        \ndef test_function(input: TestInput) -&gt; TestOutput:\n    return TestOutput(result=input.value&gt;0.5)\n\ndef test_function_batched(inputs: list[TestInput]) -&gt; list[TestOutput]:\n    return [TestOutput(result=i.value&gt;0.5) for i in inputs]\n        \nnp.random.seed(42)\nvalues = np.random.uniform(size=100).tolist()\ninputs = [TestInput(value=i) for i in values]\nexpected_outputs = [TestOutput(result=i&gt;0.5) for i in values]\n\n# thread\n\nexecutor = ThreadExecutor(test_function, batched=False, concurrency=1)\nres7 = executor(inputs)\nassert res7 == expected_outputs\n\nexecutor = ThreadExecutor(test_function, batched=False, concurrency=2)\nres8 = executor(inputs)\nassert res8 == expected_outputs\n\nexecutor = ThreadExecutor(test_function_batched, batched=True, batch_size=5)\nres9 = executor(inputs)\nassert res9 == expected_outputs\n\nexecutor = ThreadExecutor(test_function_batched, batched=True, batch_size=5, concurrency=2)\nres10 = executor(inputs)\nassert res10 == expected_outputs\n\n\nsource\n\n\nAPIExecutor\n\n APIExecutor (url:str, batch_size:int=1, concurrency:Optional[int]=1)\n\nBasic Executor class. Batches inputs, sends batches to function, unbatches outputs\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nAPI URL\n\n\nbatch_size\nint\n1\nbatch size (set batch_size=0 to pass all inputs)\n\n\nconcurrency\ntyping.Optional[int]\n1\nnumber of concurrent threads\n\n\n\n\n# only needed for jupyter\nimport nest_asyncio\nnest_asyncio.apply()\n\ninputs = [{'embedding' : np.random.randn(64).tolist(), 'item':None, 'data':None} for i in range(5)]\nexecutor = APIExecutor('http://localhost:7888/data_source', batch_size=0)\nres = executor(inputs)",
    "crumbs": [
      "executor"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nunbatch_list\n\n unbatch_list (inputs:List[List[Any]])\n\nflattens a batched list\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninputs\ntyping.List[typing.List[typing.Any]]\ninput batched list\n\n\nReturns\ntyping.List[typing.Any]\nflattened output list\n\n\n\n\nsource\n\n\nbatch_list\n\n batch_list (inputs:List[Any], batch_size:int)\n\nbatches the input list into chunks of size batch_size, with the last batch ragged\nif batch_size=0, returns list of all inputs\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninputs\ntyping.List[typing.Any]\ninput list to be batched\n\n\nbatch_size\nint\nbatch size\n\n\nReturns\ntyping.List[typing.List[typing.Any]]\nbatched output list\n\n\n\n\ninputs = list(range(10))\nassert unbatch_list(batch_list(inputs, 3)) == inputs\n\n\nsource\n\n\nbuild_batch_from_embeddings\n\n build_batch_from_embeddings (embeddings:List[List[float]])\n\ncreates a Batch from a list of embeddings. Each embedding is converted to a Query with a unique collection_id\n\n\n\n\nType\nDetails\n\n\n\n\nembeddings\ntyping.List[typing.List[float]]\ninput embeddings\n\n\nReturns\nBatch\noutput batch\n\n\n\n\nbuild_batch_from_embeddings([[0.1], [0.2]])\n\nBatch(queries=[Query(item=None, embedding=[0.1], data={}, query_results=[], internal=InteralData(removed=False, removal_reason=None, parent_id=None, collection_id=0, iteration=None), id='query_191d47ea-5809-11ee-b05f-db94e348bdfb'), Query(item=None, embedding=[0.2], data={}, query_results=[], internal=InteralData(removed=False, removal_reason=None, parent_id=None, collection_id=1, iteration=None), id='query_191d47eb-5809-11ee-b05f-db94e348bdfb')])\n\n\n\nsource\n\n\nbuild_batch_from_items\n\n build_batch_from_items (items:List[emb_opt.schemas.Item],\n                         remap_collections=False)\n\ncreates a Batch from a list of Item objects. Each Item is converted to a Query. If remap_collections=True, each Query is given a unique collection_id. Otherwise, each Query retains the collection_id of the Item used to create it\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nitems\ntyping.List[emb_opt.schemas.Item]\n\ninput items\n\n\nremap_collections\nbool\nFalse\nif collection ID should be remapped\n\n\nReturns\nBatch\n\noutput batch\n\n\n\n\nbuild_batch_from_items([Item.from_minimal(embedding=[0.1])], remap_collections=True)\n\nBatch(queries=[Query(item=None, embedding=[0.1], data={'_source_item_id': 'item_191d47ec-5809-11ee-b05f-db94e348bdfb'}, query_results=[], internal=InteralData(removed=False, removal_reason=None, parent_id=None, collection_id=0, iteration=None), id='query_191d47ed-5809-11ee-b05f-db94e348bdfb')])\n\n\n\nsource\n\n\nwhiten\n\n whiten (scores:numpy.ndarray)\n\nWhitens vector of scores\n\n\n\n\nType\nDetails\n\n\n\n\nscores\nndarray\nvector shape (n,) of scores to whiten\n\n\nReturns\nndarray\nvector shape (n,) whitened scores\n\n\n\n\nsource\n\n\nclip_grad\n\n clip_grad (grad:numpy.ndarray, max_norm:float,\n            norm_type:Union[float,int,str])\n\n\n\n\n\nType\nDetails\n\n\n\n\ngrad\nndarray\ngrad vector\n\n\nmax_norm\nfloat\nmax grad norm\n\n\nnorm_type\ntyping.Union[float, int, str]\ntype of norm to use\n\n\n\n\ngrad = np.array([1, 2, 3, 4, 5])\ngrads = np.stack([grad, grad])\nassert (clip_grad(grad, 1., 2) == clip_grad(grads, 1., 2)[0]).all()\n\n\nsource\n\n\nquery_to_rl_inputs\n\n query_to_rl_inputs (query:emb_opt.schemas.Query)\n\n\nsource\n\n\ncompute_rl_grad\n\n compute_rl_grad (query_embeddings:numpy.ndarray,\n                  result_embeddings:numpy.ndarray,\n                  result_scores:numpy.ndarray, distance_penalty:float=0,\n                  max_norm:Optional[float]=None,\n                  norm_type:Union[float,int,str,NoneType]=2.0,\n                  score_grad=False)\n\ncompute_rl_grad - uses reinforcement learning to estimate query gradients\nTo compute the gradient with RL: 1. compute advantages by whitening scores 1. advantage[i] = (scores[i] - scores.mean()) / scores.std() 2. compute advantage loss 1. advantage_loss[i] = advantage[i] * (query_embedding - result_embedding[i])**2 3. compute distance loss 1. distance_loss[i] = distance_penalty * (query_embedding - result_embedding[i])**2 4. sum loss terms 1. loss[i] = advantage_loss[i] + distance_loss[i] 5. compute the gradient\nThis gives a closed for calculation of the gradient as:\ngrad[i] = 2 * (advantage[i] + distance_penalty) * (query_embedding - result_embedding[i])\nif max_norm is specified, the gradient will be clipped using norm_type\nif score_grad=True, the sign of the gradient is flipped. The standard sign is designed for minimizing the loss via gradient descent via n_new = n_old - lr * grad. With the sign flipped, the gradient points directly in the direction of increasing score, which is conceptually aligned with hill climbing, updating via n_new = n_old + lr * grad. Use score_grad=False for anything using gradient descent.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquery_embeddings\nndarray\n\nmatrix of query embeddings\n\n\nresult_embeddings\nndarray\n\nmatrix of result embeddings\n\n\nresult_scores\nndarray\n\narray of scores\n\n\ndistance_penalty\nfloat\n0\ndistance penalty coefficient\n\n\nmax_norm\ntyping.Optional[float]\nNone\nmax gradient norm\n\n\nnorm_type\ntyping.Union[float, int, str, NoneType]\n2.0\ntype of norm to use\n\n\nscore_grad\nbool\nFalse\nif loss should be score grad or loss grad",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "runner.html",
    "href": "runner.html",
    "title": "Runner",
    "section": "",
    "text": "The Runner class holds plugin functions for each step and executes the embedding search.\n\nsource\n\nRunner\n\n Runner (data_plugin:Callable[[List[emb_opt.schemas.Query]],List[emb_opt.s\n         chemas.DataSourceResponse]], filter_plugin:Optional[Callable[[Lis\n         t[emb_opt.schemas.Item]],List[emb_opt.schemas.FilterResponse]]], \n         score_plugin:Callable[[List[emb_opt.schemas.Item]],List[emb_opt.s\n         chemas.ScoreResponse]], prune_plugin:Optional[Callable[[List[emb_\n         opt.schemas.Query]],List[emb_opt.schemas.PruneResponse]]], update\n         _plugin:Callable[[List[emb_opt.schemas.Query]],List[emb_opt.schem\n         as.UpdateResponse]])\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata_plugin\ntyping.Callable[[typing.List[emb_opt.schemas.Query]], typing.List[emb_opt.schemas.DataSourceResponse]]\ndata source function\n\n\nfilter_plugin\ntyping.Optional[typing.Callable[[typing.List[emb_opt.schemas.Item]], typing.List[emb_opt.schemas.FilterResponse]]]\noptional filter function\n\n\nscore_plugin\ntyping.Callable[[typing.List[emb_opt.schemas.Item]], typing.List[emb_opt.schemas.ScoreResponse]]\nscore function\n\n\nprune_plugin\ntyping.Optional[typing.Callable[[typing.List[emb_opt.schemas.Query]], typing.List[emb_opt.schemas.PruneResponse]]]\noptional prune function\n\n\nupdate_plugin\ntyping.Callable[[typing.List[emb_opt.schemas.Query]], typing.List[emb_opt.schemas.UpdateResponse]]\nupdate function",
    "crumbs": [
      "Runner"
    ]
  },
  {
    "objectID": "update.html",
    "href": "update.html",
    "title": "Update",
    "section": "",
    "text": "The Update step uses a set of queries and scored results to generate a new set of queries for the next iteration of the search.\nUpdates are denoted as discrete or continuous. continuous updates generate new query embeddings purely in embedding space (ie by averaging several embeddings). As a result, continuous update outputs do not have a specific item associated with them. discrete updates use a specific query result Item as the update, maintaining the item associated with it.\nThe update step is formalized by the UpdateFunction schema, which maps inputs List[Query] to outputs List[Query]. Note that the number of outputs can be different from the number of inputs.\nThe UpdateModule manages execution of a UpdateFunction. The UpdateModule gathers valid items, sends them to the UpdateFunction, and processes the results.\n\nsource\n\nUpdateModule\n\n UpdateModule (function:Callable[[List[emb_opt.schemas.Query]],List[emb_op\n               t.schemas.UpdateResponse]])\n\nModule - module base class\nGiven an input Batch, the Module: 1. gathers inputs to the function 2. executes the function 3. validates the results of the function with output_schema 4. scatters results back into the Batch\n\ndef passthrough_update_test(queries):\n    return [UpdateResponse(query=i, parent_id=None) for i in queries]\n\nbatch = Batch(queries=[\n                        Query.from_minimal(embedding=[0.1]),\n                        Query.from_minimal(embedding=[0.2]),\n                        Query.from_minimal(embedding=[0.3]),\n                    ])\n\nupdate_module = UpdateModule(passthrough_update_test)\n\nbatch = update_module(batch)\n\nassert isinstance(batch, Batch)\nassert isinstance(batch[0], Query)\n\n\ndef continuous_update_test(queries):\n    outputs = []\n    for query in queries:\n        new_query = Query.from_minimal(embedding=[i*2 for i in query.embedding])\n        outputs.append(UpdateResponse(query=new_query, parent_id=query.id))\n    return outputs\n\nbatch = Batch(queries=[\n                        Query.from_minimal(embedding=[0.1]),\n                        Query.from_minimal(embedding=[0.2]),\n                        Query.from_minimal(embedding=[0.3]),\n                    ])\n\n[batch.queries[i].update_internal(collection_id=i) for i in range(len(batch))]\n\nupdate_module = UpdateModule(continuous_update_test)\n\nbatch2 = update_module(batch)\n\nassert all([batch2[i].internal.collection_id==batch[i].internal.collection_id for i in range(len(batch2))])\n\nassert isinstance(batch2, Batch)\nassert isinstance(batch2[0], Query)\n\n\ndef discrete_update_test(queries):\n    return [UpdateResponse(query=Query.from_item(i[0]), parent_id=i.id) for i in queries]\n\nqueries = []\nfor i in range(3):\n    q = Query.from_minimal(embedding=[i*0.1])\n    q.update_internal(collection_id=i)\n    r = Item.from_minimal(embedding=[i*2*0.1])\n    q.add_query_results([r])\n    queries.append(q)\n    \nbatch = Batch(queries=queries)\n\nupdate_module = UpdateModule(discrete_update_test)\n\nbatch2 = update_module(batch)\n\nassert isinstance(batch2, Batch)\nassert isinstance(batch2[0], Query)\n\nfor i in range(len(batch2)):\n    assert batch2[i].internal.parent_id == batch[i][0].internal.parent_id\n    assert batch2[i].data['_source_item_id'] == batch[i][0].id\n    assert batch2[i].internal.collection_id == batch[i].internal.collection_id\n\n\nsource\n\n\nUpdatePlugin\n\n UpdatePlugin ()\n\nUpdatePlugin - documentation for plugin functions to UpdateFunction\nA valid UpdateFunction is any function that maps List[Query] to List[Query]. The inputs will be given as Query objects. The outputs can be either a list of Query objects or a list of valid json dictionaries that match the Query schema. The number of outputs can be different from the number of inputs\nItem schema:\n{     'id' : Optional[Union[str, int]]     'item' : Optional[Any],     'embedding' : List[float],     'score' : float,     'data' : Optional[Dict], }\nQuery schema:\n{     'item' : Optional[Any],     'embedding' : List[float],     'data' : Optional[Dict],     'query_results': List[Item] }\nUpdateResponse schema:\n{     'query' : Query,     'parent_id' : Optional[str], }\nInput schema:\nList[Query]\nOutput schema:\nList[UpdateResponse]\n\nsource\n\n\nUpdatePluginGradientWrapper\n\n UpdatePluginGradientWrapper (function:Callable[[List[emb_opt.schemas.Quer\n                              y]],List[emb_opt.schemas.UpdateResponse]],\n                              distance_penalty:float=0,\n                              max_norm:Optional[float]=None,\n                              norm_type:Union[float,int,str,NoneType]=2.0)\n\nUpdatePluginGradientWrapper - this class wraps a valid UpdateFunction to estimate the gradient of new queries using the results and scores computed for the parent query.\nThis wrapper integrates with DataPluginGradWrapper, which allows us to create new query vectors based on the gradient\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunction\ntyping.Callable[[typing.List[emb_opt.schemas.Query]], typing.List[emb_opt.schemas.UpdateResponse]]\n\nUpdateFunction to wrap\n\n\ndistance_penalty\nfloat\n0\nRL grad distance penalty\n\n\nmax_norm\ntyping.Optional[float]\nNone\nmax grad norm\n\n\nnorm_type\ntyping.Union[float, int, str, NoneType]\n2.0\ngrad norm type\n\n\n\n\nsource\n\n\nTopKDiscreteUpdate\n\n TopKDiscreteUpdate (k:int)\n\nTopKDiscreteUpdate - discrete update that generates k new queries from the top k scoring items in each input query\n\n\n\n\nType\nDetails\n\n\n\n\nk\nint\ntop k items to return as new queries\n\n\n\n\nq1 = Query.from_minimal(embedding=[0.1])\nq1.add_query_results([\n    Item(id=None, item='1', embedding=[0.11], score=-10, data=None),\n    Item(id=None, item='2', embedding=[0.12], score=6, data=None),\n    Item(id=None, item='3', embedding=[0.12], score=1, data=None),\n])\n\nq2 = Query.from_minimal(embedding=[0.2])\nq2.add_query_results([\n    Item(id=None, item='4', embedding=[0.21], score=4, data=None),\n    Item(id=None, item='5', embedding=[0.22], score=5, data=None),\n    Item(id=None, item='6', embedding=[0.12], score=2, data=None),\n])\n\nbatch = Batch(queries=[q1, q2])\n\nupdate_func = TopKDiscreteUpdate(k=2)\nupdate_module = UpdateModule(update_func)\nbatch2 = update_module(batch)\n\nassert [i.item for i in batch2] == ['2', '3', '5', '4']\n\nupdate_func2 = UpdatePluginGradientWrapper(update_func)\nupdate_module2 = UpdateModule(update_func2)\nbatch3 = update_module2(batch)\n\nq1.query_results[1].internal.removed = True\nbatch = Batch(queries=[q1, q2])\nbatch2 = update_module(batch)\nassert [i.item for i in batch2] == ['3', '1', '5', '4']\n\n\nsource\n\n\nTopKContinuousUpdate\n\n TopKContinuousUpdate (k:int)\n\nTopKContinuousUpdate - continuous update that generates 1 new query by averaging the top k scoring item embeddings for each input query\n\n\n\n\nType\nDetails\n\n\n\n\nk\nint\ntop k items to average\n\n\n\n\nq1 = Query.from_minimal(embedding=[0.1])\nq1.add_query_results([\n    Item(id=None, item='1', embedding=[0.1], score=-10, data=None),\n    Item(id=None, item='2', embedding=[0.2], score=6, data=None),\n])\n\nq2 = Query.from_minimal(embedding=[0.2])\nq2.add_query_results([\n    Item(id=None, item='4', embedding=[0.2], score=4, data=None),\n    Item(id=None, item='5', embedding=[0.3], score=5, data=None),\n])\n\nbatch = Batch(queries=[q1, q2])\n\nupdate_func = TopKContinuousUpdate(k=2)\nupdate_module = UpdateModule(update_func)\nbatch2 = update_module(batch)\n\nassert np.allclose([i.embedding for i in batch2], [[0.15], [0.25]])\n\nupdate_func = TopKContinuousUpdate(k=1)\nupdate_module = UpdateModule(update_func)\nbatch2 = update_module(batch)\n\nassert np.allclose([i.embedding for i in batch2], [[0.2], [0.3]])\n\nupdate_func2 = UpdatePluginGradientWrapper(update_func)\nupdate_module2 = UpdateModule(update_func2)\nbatch3 = update_module2(batch)\n\n\nsource\n\n\nRLUpdate\n\n RLUpdate (lrs:Union[List[float],numpy.ndarray], distance_penalty:float,\n           max_norm:Optional[float]=None,\n           norm_type:Union[float,int,str,NoneType]=2.0)\n\nRLUpdate - uses reinforcement learning to update queries\nTo compute the gradient with RL: 1. compute advantages by whitening scores 1. advantage[i] = (scores[i] - scores.mean()) / scores.std() 2. compute advantage loss 1. advantage_loss[i] = advantage[i] * (query_embedding - result_embedding[i])**2 3. compute distance loss 1. distance_loss[i] = distance_penalty * (query_embedding - result_embedding[i])**2 4. sum loss terms 1. loss[i] = advantage_loss[i] + distance_loss[i] 5. compute the gradient\nThis gives a closed for calculation of the gradient as:\ngrad[i] = 2 * (advantage[i] + distance_penalty) * (query_embedding - result_embedding[i])\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlrs\ntyping.Union[typing.List[float], numpy.ndarray]\n\nlist of learning rates\n\n\ndistance_penalty\nfloat\n\ndistance penalty coefficient\n\n\nmax_norm\ntyping.Optional[float]\nNone\noptional max grad norm for clipping\n\n\nnorm_type\ntyping.Union[float, int, str, NoneType]\n2.0\nnorm type\n\n\n\n\nlrs = np.array([1e-2, 1e-1, 1e0, 1e1])\ndp = 0.1\n\nupdate_function = RLUpdate(lrs, dp, max_norm=1.)\n\nupdate_module = UpdateModule(update_function)\n\nqueries = []\nfor i in range(1,4):\n    q = Query.from_minimal(embedding=[i*0.1, i*0.2, i*0.3, 0.1, 0.1])\n    q.update_internal(collection_id=i)\n    r1 = Item.from_minimal(embedding=[2*i*0.1, 2*i*0.2, 2*i*0.3, -0.1, -0.1], score=i*1.5)\n    r2 = Item.from_minimal(embedding=[-2*i*0.1, 2*i*0.2, -2*i*0.3, -0.1, -0.1], score=i*.5)\n    q.add_query_results([r1, r2])\n    queries.append(q)\n    \nbatch = Batch(queries=queries)\n\nbatch2 = update_module(batch)\n\nassert len(batch2)/len(batch) == len(lrs)",
    "crumbs": [
      "Update"
    ]
  },
  {
    "objectID": "tutorials/tuning.html",
    "href": "tutorials/tuning.html",
    "title": "Tuning",
    "section": "",
    "text": "Updating with a KNN method (TopKDiscreteUpdate, TopKContinuousUpdate) can result in slow convergence. This is because your effective step size is constrained by the maximum distance between your query and your query results.\nFor example, if you are querying a dense embedding space and returning the 10 nearest embeddings, the distance between your query embedding and result embeddings will be quite small. Running a KNN-style update on close embeddings results in small update steps.\nThis can be fixed by returning a larger number of query results. As a downsize, this requires more compute from your Filter and Score steps. If this is prohibitive, consider another update method",
    "crumbs": [
      "tutorials",
      "Tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#knn-convergence",
    "href": "tutorials/tuning.html#knn-convergence",
    "title": "Tuning",
    "section": "",
    "text": "Updating with a KNN method (TopKDiscreteUpdate, TopKContinuousUpdate) can result in slow convergence. This is because your effective step size is constrained by the maximum distance between your query and your query results.\nFor example, if you are querying a dense embedding space and returning the 10 nearest embeddings, the distance between your query embedding and result embeddings will be quite small. Running a KNN-style update on close embeddings results in small update steps.\nThis can be fixed by returning a larger number of query results. As a downsize, this requires more compute from your Filter and Score steps. If this is prohibitive, consider another update method",
    "crumbs": [
      "tutorials",
      "Tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#rl-update-divergence",
    "href": "tutorials/tuning.html#rl-update-divergence",
    "title": "Tuning",
    "section": "RL Update Divergence",
    "text": "RL Update Divergence\nWhen using a gradient based update method (RLUpdate), you may observe a failure mode where your query embeddings overshoot your embedding space. This can be detected by monitoring distances between queries and query results. This is a sign that your RL learning rate is too high, or your RL distance penalty is too low. It may also be worth implementing an RL Update method using more sophisticated optimization algorithms or learning rate schedules.",
    "crumbs": [
      "tutorials",
      "Tuning"
    ]
  },
  {
    "objectID": "tutorials/tuning.html#best-of-both-worlds-grad-queries-with-knn-updates",
    "href": "tutorials/tuning.html#best-of-both-worlds-grad-queries-with-knn-updates",
    "title": "Tuning",
    "section": "Best of Both Worlds: Grad Queries with KNN Updates",
    "text": "Best of Both Worlds: Grad Queries with KNN Updates\nThe above problems with KNN updates and RL updates can be simultaneously remedied using gradient queries (see UpdatePluginGradientWrapper and DataPluginGradWrapper). With this setup, we use a score gradient to generate multiple queries, but update with a KNN method.\nThis allows us to be very aggressive with our gradient query, sweeping a large range of learning rates and intentionally overshooting our embedding space. The KNN update then ensures that our new queries are pulled back into the embedding space.",
    "crumbs": [
      "tutorials",
      "Tuning"
    ]
  },
  {
    "objectID": "score.html",
    "href": "score.html",
    "title": "Score",
    "section": "",
    "text": "The Score step assigns a numeric score to each Item result. This score is used to drive the hill climbing algorithm. The score step is formalized by the ScoreFunction schema, which maps inputs List[Item] to outputs List[ScoreResponse].\nThe ScoreModule manages execution of a ScoreFunction. The ScoreModule gathers valid items, sends them to the ScoreFunction, and processes the results.\n\nsource\n\nScoreModule\n\n ScoreModule (function:Callable[[List[emb_opt.schemas.Item]],List[emb_opt.\n              schemas.ScoreResponse]])\n\nModule - module base class\nGiven an input Batch, the Module: 1. gathers inputs to the function 2. executes the function 3. validates the results of the function with output_schema 4. scatters results back into the Batch\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunction\ntyping.Callable[[typing.List[emb_opt.schemas.Item]], typing.List[emb_opt.schemas.ScoreResponse]]\nscore function\n\n\n\n\ndef build_batch():\n    d_emb = 128\n    n_emb = 100\n    np.random.seed(42)\n    \n    embeddings = np.random.randn(n_emb+1, d_emb)\n    query = Query.from_minimal(embedding=embeddings[-1])\n    results = [Item.from_minimal(id=i, embedding=embeddings[i]) for i in range(n_emb)]\n    query.add_query_results(results)\n    batch = Batch(queries=[query])\n    \n    expected_scores = [np.linalg.norm(i.embedding) for i in results]\n    return batch, expected_scores\n\nclass NormScore():\n    def __init__(self, test_fails, test_nulls):\n        self.test_fails = test_fails\n        self.test_nulls = test_nulls\n        \n    def __call__(self, inputs: List[Item]) -&gt; List[ScoreResponse]:\n        embeddings = np.array([i.embedding for i in inputs])\n        norms = np.linalg.norm(embeddings, axis=-1)\n        \n        if self.test_fails:\n            results = [ScoreResponse(valid=False, score=i, data={'norm':i}) for i in norms]\n        elif self.test_nulls:\n            results = [ScoreResponse(valid=True, score=None if np.random.uniform()&lt;0.5 else i, \n                                     data={'norm':i}) for i in norms]\n        else:\n            results = [ScoreResponse(valid=True, score=i, data={'norm':i}) for i in norms]\n            \n        return results\n    \nbatch, scores = build_batch()\nscore_func = NormScore(False, False)\nscore_module = ScoreModule(score_func)\nbatch = score_module(batch)\nassert np.allclose([i.score for i in batch[0]], scores)\n\nbatch, scores = build_batch()\nscore_func = NormScore(True, False)\nscore_module = ScoreModule(score_func)\nbatch = score_module(batch)\nassert batch[0].internal.removed\n\nbatch, scores = build_batch()\nscore_func = NormScore(False, True)\nscore_module = ScoreModule(score_func)\nbatch = score_module(batch)\n\nfor _, result in batch.enumerate_query_results(skip_removed=False):\n    if result.score is None:\n        assert result.internal.removed\n    else:\n        assert not result.internal.removed\n\n\nsource\n\n\nScorePlugin\n\n ScorePlugin ()\n\nScorePlugin - documentation for plugin functions to ScoreFunction\nA valid ScoreFunction is any function that maps List[Item] to List[ScoreResponse]. The inputs will be given as Item objects. The outputs can be either a list of ScoreResponse objects or a list of valid json dictionaries that match the ScoreResponse schema.\nItem schema:\n{     'id' : Optional[Union[str, int]]     'item' : Optional[Any],     'embedding' : List[float],     'score' : None, # will be None at this stage     'data' : Optional[Dict], }\nInput schema:\nList[Item]\nScoreResponse schema:\n{     'valid' : bool,     'score' : Optional[float], # can be None if valid=False     'data' : Optional[Dict], }\nOutput schema:\nList[ScoreResponse]\nThe CompositeScorePlugin can be used to chain together a list of valid ScoreFunction\n\nsource\n\n\nCompositeScorePlugin\n\n CompositeScorePlugin (functions:List[Callable[[List[emb_opt.schemas.Item]\n                       ],List[emb_opt.schemas.ScoreResponse]]])\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunctions\ntyping.List[typing.Callable[[typing.List[emb_opt.schemas.Item]], typing.List[emb_opt.schemas.ScoreResponse]]]\nlist of score functions\n\n\n\n\nd_emb = 128\nn_emb = 100\nnp.random.seed(42)\n\nembeddings = np.random.randn(n_emb+1, d_emb)\nquery = Query.from_minimal(embedding=embeddings[-1])\nresults = [Item.from_minimal(id=i, embedding=embeddings[i]) for i in range(n_emb)]\nquery.add_query_results(results)\nbatch = Batch(queries=[query])\n\ndef norm_score(inputs: List[Item]) -&gt; List[ScoreResponse]:\n    embeddings = np.array([i.embedding for i in inputs])\n    norms = np.linalg.norm(embeddings, axis=-1)\n    return [ScoreResponse(valid=True, score=i, data={'norm':i}) for i in norms]\n\ndef sum_score(inputs: List[Item]) -&gt; List[ScoreResponse]:\n    embeddings = np.array([i.embedding for i in inputs])\n    sums = embeddings.sum(-1)\n    return [ScoreResponse(valid=True, score=i, data={'sum':i}) for i in sums]\n\nscore_function = CompositeScorePlugin([norm_score, sum_score])\nscore_module = ScoreModule(score_function)\n\nbatch = score_module(batch)",
    "crumbs": [
      "Score"
    ]
  }
]