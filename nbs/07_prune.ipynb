{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune\n",
    "\n",
    "> Prune functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from emb_opt.imports import *\n",
    "from emb_opt.module import Module\n",
    "from emb_opt.schemas import Item, Query, Batch, PruneFunction, PruneResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Prune step optionally removes queries prior to the update step. A Prune step allows for control over the total number of queries in the scenario where the Update step generates multiple output queries for each input.\n",
    "\n",
    "The prune step is formalized by the `PruneFunction` schema, which maps inputs `List[Query]` to outputs `List[PruneResponse]`.\n",
    "\n",
    "The `PruneModule` manages execution of a `PruneFunction`. The `PruneModule` gathers valid items, sends them to the `PruneFunction`, and processes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PruneModule(Module):\n",
    "    def __init__(self,\n",
    "                 function: PruneFunction # prune function\n",
    "                ):\n",
    "        super().__init__(PruneResponse, function)\n",
    "        \n",
    "    def gather_inputs(self, batch: Batch) -> (List[Tuple], List[Query]):\n",
    "        idxs, inputs = batch.flatten_queries()\n",
    "        return (idxs, inputs)\n",
    "    \n",
    "    def scatter_results(self, batch: Batch, idxs: List[Tuple], results: List[PruneResponse]):\n",
    "        for (q_idx, r_idx), result in zip(idxs, results):\n",
    "            batch_item = batch.get_item(q_idx, r_idx)\n",
    "            if result.data:\n",
    "                batch_item.data.update(result.data)\n",
    "\n",
    "            if not result.valid:\n",
    "                batch_item.update_internal(removed=True, removal_reason='prune response invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch(queries=[\n",
    "                        Query.from_minimal(embedding=[0.1]),\n",
    "                        Query.from_minimal(embedding=[0.2]),\n",
    "                        Query.from_minimal(embedding=[0.3]),\n",
    "                    ])\n",
    "\n",
    "def prune_func(queries):\n",
    "    return [PruneResponse(valid=i.embedding[0]>=0.2, data=None) for i in queries]\n",
    "\n",
    "prune_module = PruneModule(prune_func)\n",
    "\n",
    "batch = prune_module(batch)\n",
    "\n",
    "assert [i.internal.removed for i in batch] == [True, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PrunePlugin():\n",
    "    '''\n",
    "    PrunePlugin - documentation for plugin functions to `PruneFunction`\n",
    "    \n",
    "    A valid `PruneFunction` is any function that maps `List[Query]` to \n",
    "    `List[PruneResponse]`. The inputs will be given as `Query` objects. \n",
    "    The outputs can be either a list of `PruneResponse` objects or a list of \n",
    "    valid json dictionaries that match the `PruneResponse` schema\n",
    "    \n",
    "    The Prune step is called after scoring, so each result `Item` in the \n",
    "    input queries will have a score assigned\n",
    "    \n",
    "    Item schema:\n",
    "    \n",
    "    `{\n",
    "        'id' : Optional[Union[str, int]]\n",
    "        'item' : Optional[Any],\n",
    "        'embedding' : List[float],\n",
    "        'score' : float,\n",
    "        'data' : Optional[Dict],\n",
    "    }`\n",
    "    \n",
    "    \n",
    "    Query schema:\n",
    "    \n",
    "    `{\n",
    "        'item' : Optional[Any],\n",
    "        'embedding' : List[float],\n",
    "        'data' : Optional[Dict],\n",
    "        'query_results': List[Item]\n",
    "    }`\n",
    "    \n",
    "    Input schema:\n",
    "    \n",
    "    `List[Query]`\n",
    "    \n",
    "    PruneResponse schema:\n",
    "    \n",
    "    `{\n",
    "        'valid' : bool,\n",
    "        'data' : Optional[Dict],\n",
    "    }`\n",
    "    \n",
    "    Output schema:\n",
    "    \n",
    "    `List[PruneResponse]`\n",
    "    \n",
    "    '''\n",
    "    def __call__(self, inputs: List[Query]) -> List[PruneResponse]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TopKPrune():\n",
    "    '''\n",
    "    TopKPrune - keeps the top `k` best queries in each group \n",
    "    by aggregated score\n",
    "    \n",
    "    queries are first grouped by `group_by`\n",
    "    * if `group_by=None`, all queries are considered the same group (global pruning)\n",
    "    * if `group_by='parent_id'`, queries are grouped by parent query id\n",
    "    * if `group_by='collection_id', queries are grouped by collection id\n",
    "    \n",
    "    queries are then assigned a score based on aggregating query result scores\n",
    "    * if `score_agg='mean'`, each `Query` is scored by the average score of all `Item` results\n",
    "    * if `score_agg='max'`, each `Query` is scored by the max scoring `Item` result\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 k: int,\n",
    "                 score_agg: str='mean', # ['mean', 'max']\n",
    "                 group_by: Optional[str]='collection_id' # [None, 'collection_id', 'parent_id']\n",
    "                ):\n",
    "        self.k = k\n",
    "        self.score_agg = score_agg\n",
    "        self.group_by = group_by\n",
    "        \n",
    "        assert self.score_agg in ['mean', 'max']\n",
    "        assert self.group_by in [None, 'collection_id', 'parent_id']\n",
    "        \n",
    "    def agg_scores(self, query: Query):\n",
    "        result_scores = np.array([i.score for i in query.valid_results()])\n",
    "        if self.score_agg == 'mean':\n",
    "            result_scores = result_scores.mean()\n",
    "        elif self.score_agg == 'max':\n",
    "            result_scores = result_scores.max()\n",
    "        return result_scores\n",
    "    \n",
    "    def get_group_key(self, query: Query):\n",
    "        key = None\n",
    "        if self.group_by == 'collection_id':\n",
    "            key = query.internal.collection_id\n",
    "        elif self.group_by == 'parent_id':\n",
    "            key = query.internal.parent_id\n",
    "        return key\n",
    "        \n",
    "    def prune_queries(self, queries: List[Query]) -> List[PruneResponse]:\n",
    "        scores = []\n",
    "        for query in queries:\n",
    "            scores.append(self.agg_scores(query))\n",
    "            \n",
    "        scores = np.array(scores)\n",
    "        topk_idxs = set(scores.argsort()[::-1][:self.k])\n",
    "        \n",
    "        outputs = [PruneResponse(valid=(i in topk_idxs), data={f'{self.score_agg}_score':scores[i]})\n",
    "                  for i in range(len(queries))]\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def group_queries(self, queries: List[Query]) -> (dict, dict):\n",
    "        query_groups = defaultdict(list)\n",
    "        idx_groups = defaultdict(list)\n",
    "        \n",
    "        for query_idx, query in enumerate(queries):\n",
    "            key = self.get_group_key(query)\n",
    "            query_groups[key].append(query)\n",
    "            idx_groups[key].append(query_idx)\n",
    "            \n",
    "        return idx_groups, query_groups\n",
    "    \n",
    "    def scatter_queries(self, \n",
    "                        idx_groups: dict, \n",
    "                        query_groups: dict, \n",
    "                        total_queries: int)  -> List[PruneResponse]:\n",
    "        \n",
    "        outputs = [None for i in range(total_queries)]\n",
    "        \n",
    "        for key, query_list in query_groups.items():\n",
    "            prune_results = self.prune_queries(query_list)\n",
    "            scatter_idxs = idx_groups[key]\n",
    "            \n",
    "            for i, result in enumerate(prune_results):\n",
    "                outputs[scatter_idxs[i]] = result\n",
    "                \n",
    "        return outputs\n",
    "    \n",
    "    def __call__(self, queries: List[Query]) -> List[PruneResponse]:\n",
    "        idx_groups, query_groups = self.group_queries(queries)\n",
    "        results = self.scatter_queries(idx_groups, query_groups, len(queries))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Query.from_minimal(embedding=[0.1])\n",
    "q1.update_internal(collection_id=0)\n",
    "q1.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.11], score=-10),\n",
    "    Item.from_minimal(embedding=[0.12], score=6),\n",
    "])\n",
    "\n",
    "q2 = Query.from_minimal(embedding=[0.2])\n",
    "q2.update_internal(collection_id=0)\n",
    "q2.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.21], score=4),\n",
    "    Item.from_minimal(embedding=[0.22], score=5),\n",
    "])\n",
    "\n",
    "q3 = Query.from_minimal(embedding=[0.3])\n",
    "q3.update_internal(collection_id=1)\n",
    "q3.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.31], score=7),\n",
    "    Item.from_minimal(embedding=[0.32], score=8),\n",
    "])\n",
    "\n",
    "queries = [q1, q2, q3]\n",
    "\n",
    "prune_func = TopKPrune(k=1, score_agg='mean', group_by=None)\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Query.from_minimal(embedding=[0.1])\n",
    "q1.update_internal(collection_id=0)\n",
    "q1.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.11], score=-10),\n",
    "    Item.from_minimal(embedding=[0.12], score=6),\n",
    "])\n",
    "\n",
    "q2 = Query.from_minimal(embedding=[0.2])\n",
    "q2.update_internal(collection_id=0)\n",
    "q2.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.21], score=4),\n",
    "    Item.from_minimal(embedding=[0.22], score=5),\n",
    "])\n",
    "\n",
    "q3 = Query.from_minimal(embedding=[0.3])\n",
    "q3.update_internal(collection_id=1)\n",
    "q3.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.31], score=7),\n",
    "    Item.from_minimal(embedding=[0.32], score=8),\n",
    "])\n",
    "\n",
    "queries = [q1, q2, q3]\n",
    "\n",
    "prune_func = TopKPrune(k=1, score_agg='max', group_by='collection_id')\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [True, False, True]\n",
    "\n",
    "prune_func = TopKPrune(k=1, score_agg='mean', group_by='collection_id')\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [False, True, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_opt",
   "language": "python",
   "name": "emb_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
