{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune\n",
    "\n",
    "> Prune functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from emb_opt.imports import *\n",
    "from emb_opt.module import Module\n",
    "from emb_opt.schemas import Item, Query, Batch, PruneFunction, PruneResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PruneModule(Module):\n",
    "    def __init__(self,\n",
    "                 function: PruneFunction,\n",
    "                ):\n",
    "        super().__init__(PruneResponse, function)\n",
    "        \n",
    "    def gather_inputs(self, batch: Batch) -> (List[Tuple], List[Query]):\n",
    "        idxs, inputs = batch.flatten_queries()\n",
    "        return (idxs, inputs)\n",
    "    \n",
    "    def scatter_results(self, batch: Batch, idxs: List[Tuple], results: List[PruneResponse]):\n",
    "        for (q_idx, r_idx), result in zip(idxs, results):\n",
    "            batch_item = batch.get_item(q_idx, r_idx)\n",
    "            if result.data:\n",
    "                batch_item.data.update(result.data)\n",
    "\n",
    "            if not result.valid:\n",
    "                batch_item.update_internal(removed=True, removal_reason='prune response invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch(queries=[\n",
    "                        Query(embedding=[0.1]),\n",
    "                        Query(embedding=[0.2]),\n",
    "                        Query(embedding=[0.3]),\n",
    "                    ])\n",
    "\n",
    "def prune_func(queries):\n",
    "    return [PruneResponse(valid=i.embedding[0]>=0.2) for i in queries]\n",
    "\n",
    "prune_module = PruneModule(prune_func)\n",
    "\n",
    "batch = prune_module(batch)\n",
    "\n",
    "assert [i.internal.removed for i in batch] == [True, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PrunePlugin():\n",
    "    def __call__(self, inputs: List[Query]) -> List[PruneResponse]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TopKGlobalPrune():\n",
    "    def __init__(self,\n",
    "                 k: int,\n",
    "                 agg: str='mean'\n",
    "                ):\n",
    "        self.k = k\n",
    "        self.agg = agg\n",
    "        assert self.agg in ['mean', 'max']\n",
    "        \n",
    "    def prune_queries(self, queries: List[Query]) -> List[PruneResponse]:\n",
    "        scores = []\n",
    "        for query in queries:\n",
    "            result_scores = np.array([i.score for i in query.valid_results()])\n",
    "            if self.agg=='mean':\n",
    "                result_scores = result_scores.mean()\n",
    "            elif self.agg == 'max':\n",
    "                result_scores = result_scores.max()\n",
    "            scores.append(result_scores)\n",
    "            \n",
    "        scores = np.array(scores)\n",
    "        topk_idxs = set(scores.argsort()[::-1][:self.k])\n",
    "        \n",
    "        outputs = [PruneResponse(valid=(i in topk_idxs), data={f'{self.agg}_score':scores[i]})\n",
    "                  for i in range(len(queries))]\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def __call__(self, queries: List[Query]) -> List[PruneResponse]:\n",
    "        outputs = self.prune_queries(queries)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Query(embedding=[0.1])\n",
    "q1.update_internal(collection_id=0)\n",
    "q1.add_query_results([\n",
    "    Item(embedding=[0.11], score=-10),\n",
    "    Item(embedding=[0.12], score=6),\n",
    "])\n",
    "\n",
    "q2 = Query(embedding=[0.2])\n",
    "q2.update_internal(collection_id=0)\n",
    "q2.add_query_results([\n",
    "    Item(embedding=[0.21], score=4),\n",
    "    Item(embedding=[0.22], score=5),\n",
    "])\n",
    "\n",
    "q3 = Query(embedding=[0.3])\n",
    "q3.update_internal(collection_id=1)\n",
    "q3.add_query_results([\n",
    "    Item(embedding=[0.31], score=7),\n",
    "    Item(embedding=[0.32], score=8),\n",
    "])\n",
    "\n",
    "queries = [q1, q2, q3]\n",
    "\n",
    "prune_func = TopKGlobalPrune(k=1, agg='mean')\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKPruneLocal(TopKGlobalPrune):\n",
    "    def __call__(self, queries: List[Query]) -> List[PruneResponse]:\n",
    "        query_groups = defaultdict(list)\n",
    "        idx_groups = defaultdict(list)\n",
    "        \n",
    "        outputs = [None for i in queries]\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            collection_id = query.internal.collection_id\n",
    "            query_groups[collection_id].append(query)\n",
    "            idx_groups[collection_id].append(i)\n",
    "            \n",
    "        for collection_id, query_list in query_groups.items():\n",
    "            prune_results = self.prune_queries(query_list)\n",
    "            scatter_idxs = idx_groups[collection_id]\n",
    "            \n",
    "            for i, result in enumerate(prune_results):\n",
    "                outputs[scatter_idxs[i]] = result\n",
    "                    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Query(embedding=[0.1])\n",
    "q1.update_internal(collection_id=0)\n",
    "q1.add_query_results([\n",
    "    Item(embedding=[0.11], score=-10),\n",
    "    Item(embedding=[0.12], score=6),\n",
    "])\n",
    "\n",
    "q2 = Query(embedding=[0.2])\n",
    "q2.update_internal(collection_id=0)\n",
    "q2.add_query_results([\n",
    "    Item(embedding=[0.21], score=4),\n",
    "    Item(embedding=[0.22], score=5),\n",
    "])\n",
    "\n",
    "q3 = Query(embedding=[0.3])\n",
    "q3.update_internal(collection_id=1)\n",
    "q3.add_query_results([\n",
    "    Item(embedding=[0.31], score=7),\n",
    "    Item(embedding=[0.32], score=8),\n",
    "])\n",
    "\n",
    "queries = [q1, q2, q3]\n",
    "\n",
    "prune_func = TopKPruneLocal(k=1, agg='max')\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [True, False, True]\n",
    "\n",
    "prune_func = TopKPruneLocal(k=1, agg='mean')\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [False, True, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_opt",
   "language": "python",
   "name": "emb_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
