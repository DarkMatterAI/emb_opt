{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune\n",
    "\n",
    "> Prune functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from emb_opt.imports import *\n",
    "from emb_opt.module import Module\n",
    "from emb_opt.schemas import Item, Query, Batch, PruneFunction, PruneResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Prune step optionally removes queries prior to the update step. A Prune step allows for control over the total number of queries in the scenario where the Update step generates multiple output queries for each input.\n",
    "\n",
    "The prune step is formalized by the `PruneFunction` schema, which maps inputs `List[Query]` to outputs `List[PruneResponse]`.\n",
    "\n",
    "The `PruneModule` manages execution of a `PruneFunction`. The `PruneModule` gathers valid items, sends them to the `PruneFunction`, and processes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PruneModule(Module):\n",
    "    def __init__(self,\n",
    "                 function: PruneFunction # prune function\n",
    "                ):\n",
    "        super().__init__(PruneResponse, function)\n",
    "        \n",
    "    def gather_inputs(self, batch: Batch) -> (List[Tuple], List[Query]):\n",
    "        idxs, inputs = batch.flatten_queries()\n",
    "        return (idxs, inputs)\n",
    "    \n",
    "    def scatter_results(self, batch: Batch, idxs: List[Tuple], results: List[PruneResponse]):\n",
    "        for (q_idx, r_idx), result in zip(idxs, results):\n",
    "            batch_item = batch.get_item(q_idx, r_idx)\n",
    "            if result.data:\n",
    "                batch_item.data.update(result.data)\n",
    "\n",
    "            if not result.valid:\n",
    "                batch_item.update_internal(removed=True, removal_reason='prune response invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch(queries=[\n",
    "                        Query.from_minimal(embedding=[0.1]),\n",
    "                        Query.from_minimal(embedding=[0.2]),\n",
    "                        Query.from_minimal(embedding=[0.3]),\n",
    "                    ])\n",
    "\n",
    "def prune_func(queries):\n",
    "    return [PruneResponse(valid=i.embedding[0]>=0.2, data=None) for i in queries]\n",
    "\n",
    "prune_module = PruneModule(prune_func)\n",
    "\n",
    "batch = prune_module(batch)\n",
    "\n",
    "assert [i.internal.removed for i in batch] == [True, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PrunePlugin():\n",
    "    '''\n",
    "    PrunePlugin - documentation for plugin functions to `PruneFunction`\n",
    "    \n",
    "    A valid `PruneFunction` is any function that maps `List[Query]` to \n",
    "    `List[PruneResponse]`. The inputs will be given as `Query` objects. \n",
    "    The outputs can be either a list of `PruneResponse` objects or a list of \n",
    "    valid json dictionaries that match the `PruneResponse` schema\n",
    "    \n",
    "    The Prune step is called after scoring, so each result `Item` in the \n",
    "    input queries will have a score assigned\n",
    "    \n",
    "    Item schema:\n",
    "    \n",
    "    `{\n",
    "        'id' : Optional[Union[str, int]]\n",
    "        'item' : Optional[Any],\n",
    "        'embedding' : List[float],\n",
    "        'score' : float,\n",
    "        'data' : Optional[Dict],\n",
    "    }`\n",
    "    \n",
    "    \n",
    "    Query schema:\n",
    "    \n",
    "    `{\n",
    "        'item' : Optional[Any],\n",
    "        'embedding' : List[float],\n",
    "        'data' : Optional[Dict],\n",
    "        'query_results': List[Item]\n",
    "    }`\n",
    "    \n",
    "    Input schema:\n",
    "    \n",
    "    `List[Query]`\n",
    "    \n",
    "    PruneResponse schema:\n",
    "    \n",
    "    `{\n",
    "        'valid' : bool,\n",
    "        'data' : Optional[Dict],\n",
    "    }`\n",
    "    \n",
    "    Output schema:\n",
    "    \n",
    "    `List[PruneResponse]`\n",
    "    \n",
    "    '''\n",
    "    def __call__(self, inputs: List[Query]) -> List[PruneResponse]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TopKGlobalPrune():\n",
    "    '''\n",
    "    TopKGlobalPrune - keeps the top `k` best queries \n",
    "    by aggregated score, removing the rest.\n",
    "    \n",
    "    If `agg='mean'`, each `Query` is scored by the average \n",
    "    score of all `Item` results in the `Query`.\n",
    "    \n",
    "    If `agg='max'`, each `Query` is scored by the max \n",
    "    score of all `Item` results in the `Query`. \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 k: int,         # top k queries to keep\n",
    "                 agg: str='mean' # score aggregation method, should be one of ['mean', 'max']\n",
    "                ):\n",
    "        self.k = k\n",
    "        self.agg = agg\n",
    "        assert self.agg in ['mean', 'max']\n",
    "        \n",
    "    def prune_queries(self, queries: List[Query]) -> List[PruneResponse]:\n",
    "        scores = []\n",
    "        for query in queries:\n",
    "            result_scores = np.array([i.score for i in query.valid_results()])\n",
    "            if self.agg=='mean':\n",
    "                result_scores = result_scores.mean()\n",
    "            elif self.agg == 'max':\n",
    "                result_scores = result_scores.max()\n",
    "            scores.append(result_scores)\n",
    "            \n",
    "        scores = np.array(scores)\n",
    "        topk_idxs = set(scores.argsort()[::-1][:self.k])\n",
    "        \n",
    "        outputs = [PruneResponse(valid=(i in topk_idxs), data={f'{self.agg}_score':scores[i]})\n",
    "                  for i in range(len(queries))]\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def __call__(self, queries: List[Query]) -> List[PruneResponse]:\n",
    "        outputs = self.prune_queries(queries)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Query.from_minimal(embedding=[0.1])\n",
    "q1.update_internal(collection_id=0)\n",
    "q1.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.11], score=-10),\n",
    "    Item.from_minimal(embedding=[0.12], score=6),\n",
    "])\n",
    "\n",
    "q2 = Query.from_minimal(embedding=[0.2])\n",
    "q2.update_internal(collection_id=0)\n",
    "q2.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.21], score=4),\n",
    "    Item.from_minimal(embedding=[0.22], score=5),\n",
    "])\n",
    "\n",
    "q3 = Query.from_minimal(embedding=[0.3])\n",
    "q3.update_internal(collection_id=1)\n",
    "q3.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.31], score=7),\n",
    "    Item.from_minimal(embedding=[0.32], score=8),\n",
    "])\n",
    "\n",
    "queries = [q1, q2, q3]\n",
    "\n",
    "prune_func = TopKGlobalPrune(k=1, agg='mean')\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TopKPruneLocal(TopKGlobalPrune):\n",
    "    '''\n",
    "    TopKPruneLocal - keeps the top `k` best queries \n",
    "    within each `collection_id` group by aggregated score, \n",
    "    removing the rest.\n",
    "    \n",
    "    If `agg='mean'`, each `Query` is scored by the average \n",
    "    score of all `Item` results in the `Query`.\n",
    "    \n",
    "    If `agg='max'`, each `Query` is scored by the max \n",
    "    score of all `Item` results in the `Query`. \n",
    "    '''\n",
    "    def __call__(self, queries: List[Query]) -> List[PruneResponse]:\n",
    "        query_groups = defaultdict(list)\n",
    "        idx_groups = defaultdict(list)\n",
    "        \n",
    "        outputs = [None for i in queries]\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            collection_id = query.internal.collection_id\n",
    "            query_groups[collection_id].append(query)\n",
    "            idx_groups[collection_id].append(i)\n",
    "            \n",
    "        for collection_id, query_list in query_groups.items():\n",
    "            prune_results = self.prune_queries(query_list)\n",
    "            scatter_idxs = idx_groups[collection_id]\n",
    "            \n",
    "            for i, result in enumerate(prune_results):\n",
    "                outputs[scatter_idxs[i]] = result\n",
    "                    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = Query.from_minimal(embedding=[0.1])\n",
    "q1.update_internal(collection_id=0)\n",
    "q1.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.11], score=-10),\n",
    "    Item.from_minimal(embedding=[0.12], score=6),\n",
    "])\n",
    "\n",
    "q2 = Query.from_minimal(embedding=[0.2])\n",
    "q2.update_internal(collection_id=0)\n",
    "q2.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.21], score=4),\n",
    "    Item.from_minimal(embedding=[0.22], score=5),\n",
    "])\n",
    "\n",
    "q3 = Query.from_minimal(embedding=[0.3])\n",
    "q3.update_internal(collection_id=1)\n",
    "q3.add_query_results([\n",
    "    Item.from_minimal(embedding=[0.31], score=7),\n",
    "    Item.from_minimal(embedding=[0.32], score=8),\n",
    "])\n",
    "\n",
    "queries = [q1, q2, q3]\n",
    "\n",
    "prune_func = TopKPruneLocal(k=1, agg='max')\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [True, False, True]\n",
    "\n",
    "prune_func = TopKPruneLocal(k=1, agg='mean')\n",
    "\n",
    "assert [i.valid for i in prune_func(queries)] == [False, True, True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CompositePrunePlugin` can be used to chain together a list of valid `PruneFunction`\n",
    "\n",
    "\"isolated\" prune vs \"population\" prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CompositePrunePlugin():\n",
    "    def __init__(self, \n",
    "                 functions: List[PruneFunction] # list of prune functions\n",
    "                ):\n",
    "        self.functions = functions\n",
    "        \n",
    "    def __call__(self, inputs: List[Query]) -> List[PruneResponse]:\n",
    "        results = [func(inputs) for func in self.functions]\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            data = {'score_results' : [result[i].model_dump() for result in results]}\n",
    "            valid = all([result[i].valid for result in results])\n",
    "            score = sum([result[i].score for result in results])\n",
    "                \n",
    "            outputs.append(ScoreResponse(valid=valid, score=score, data=data))\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruneResponse(BaseModel):\n",
    "    valid: bool           \n",
    "    data: Optional[Dict] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_opt",
   "language": "python",
   "name": "emb_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
