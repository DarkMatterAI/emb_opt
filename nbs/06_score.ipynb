{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score\n",
    "\n",
    "> Score functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from emb_opt.imports import *\n",
    "from emb_opt.module import Module\n",
    "from emb_opt.schemas import Item, Query, Batch, ScoreFunction, ScoreResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Score step assigns a numeric score to each `Item` result. This score is used to drive the hill climbing algorithm. The score step is formalized by the `ScoreFunction` schema, which maps inputs `List[Item]` to outputs `List[ScoreResponse]`.\n",
    "\n",
    "The `ScoreModule` manages execution of a `ScoreFunction`. The `ScoreModule` gathers valid items, sends them to the `ScoreFunction`, and processes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ScoreModule(Module):\n",
    "    def __init__(self, \n",
    "                 function: ScoreFunction # score function\n",
    "                ):\n",
    "        super().__init__(ScoreResponse, function)\n",
    "        \n",
    "    def gather_inputs(self, batch: Batch) -> (List[Tuple], List[Item]):\n",
    "        idxs, inputs = batch.flatten_query_results()\n",
    "        return (idxs, inputs)\n",
    "    \n",
    "    def scatter_results(self, batch: Batch, idxs: List[Tuple], results: List[ScoreResponse]):\n",
    "        for (q_idx, r_idx), result in zip(idxs, results):\n",
    "            batch_item = batch.get_item(q_idx, r_idx)\n",
    "            \n",
    "            batch_item.score = result.score\n",
    "            \n",
    "            if result.data:\n",
    "                batch_item.data.update(result.data)\n",
    "                \n",
    "            if not result.valid:\n",
    "                batch_item.update_internal(removed=True, removal_reason='score response invalid')\n",
    "                \n",
    "        for _, result in batch.enumerate_query_results():\n",
    "            if (result.score is None) and (not result.internal.removed):\n",
    "                result.update_internal(removed=True, removal_reason='null score')\n",
    "                \n",
    "        for query in batch:\n",
    "            query.update_internal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch():\n",
    "    d_emb = 128\n",
    "    n_emb = 100\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    embeddings = np.random.randn(n_emb+1, d_emb)\n",
    "    query = Query.from_minimal(embedding=embeddings[-1])\n",
    "    results = [Item.from_minimal(id=i, embedding=embeddings[i]) for i in range(n_emb)]\n",
    "    query.add_query_results(results)\n",
    "    batch = Batch(queries=[query])\n",
    "    \n",
    "    expected_scores = [np.linalg.norm(i.embedding) for i in results]\n",
    "    return batch, expected_scores\n",
    "\n",
    "class NormScore():\n",
    "    def __init__(self, test_fails, test_nulls):\n",
    "        self.test_fails = test_fails\n",
    "        self.test_nulls = test_nulls\n",
    "        \n",
    "    def __call__(self, inputs: List[Item]) -> List[ScoreResponse]:\n",
    "        embeddings = np.array([i.embedding for i in inputs])\n",
    "        norms = np.linalg.norm(embeddings, axis=-1)\n",
    "        \n",
    "        if self.test_fails:\n",
    "            results = [ScoreResponse(valid=False, score=i, data={'norm':i}) for i in norms]\n",
    "        elif self.test_nulls:\n",
    "            results = [ScoreResponse(valid=True, score=None if np.random.uniform()<0.5 else i, \n",
    "                                     data={'norm':i}) for i in norms]\n",
    "        else:\n",
    "            results = [ScoreResponse(valid=True, score=i, data={'norm':i}) for i in norms]\n",
    "            \n",
    "        return results\n",
    "    \n",
    "batch, scores = build_batch()\n",
    "score_func = NormScore(False, False)\n",
    "score_module = ScoreModule(score_func)\n",
    "batch = score_module(batch)\n",
    "assert np.allclose([i.score for i in batch[0]], scores)\n",
    "\n",
    "batch, scores = build_batch()\n",
    "score_func = NormScore(True, False)\n",
    "score_module = ScoreModule(score_func)\n",
    "batch = score_module(batch)\n",
    "assert batch[0].internal.removed\n",
    "\n",
    "batch, scores = build_batch()\n",
    "score_func = NormScore(False, True)\n",
    "score_module = ScoreModule(score_func)\n",
    "batch = score_module(batch)\n",
    "\n",
    "for _, result in batch.enumerate_query_results(skip_removed=False):\n",
    "    if result.score is None:\n",
    "        assert result.internal.removed\n",
    "    else:\n",
    "        assert not result.internal.removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ScorePlugin():\n",
    "    '''\n",
    "    ScorePlugin - documentation for plugin functions to `ScoreFunction`\n",
    "    \n",
    "    A valid `ScoreFunction` is any function that maps `List[Item]` to\n",
    "    `List[ScoreResponse]`. The inputs will be given as `Item` objects. \n",
    "    The outputs can be either a list of `ScoreResponse` objects or a list \n",
    "    of valid json dictionaries that match the `ScoreResponse` schema.\n",
    "    \n",
    "    Item schema:\n",
    "    \n",
    "    `{\n",
    "        'id' : Optional[Union[str, int]]\n",
    "        'item' : Optional[Any],\n",
    "        'embedding' : List[float],\n",
    "        'score' : None, # will be None at this stage\n",
    "        'data' : Optional[Dict],\n",
    "    }`\n",
    "    \n",
    "    Input schema:\n",
    "    \n",
    "    `List[Item]`\n",
    "    \n",
    "    ScoreResponse schema:\n",
    "\n",
    "    `{\n",
    "        'valid' : bool,\n",
    "        'score' : Optional[float], # can be None if valid=False\n",
    "        'data' : Optional[Dict],\n",
    "    }`\n",
    "    \n",
    "    Output schema:\n",
    "    \n",
    "    `List[ScoreResponse]`\n",
    "    \n",
    "    '''\n",
    "    def __call__(self, inputs: List[Query]) -> List[ScoreResponse]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_opt",
   "language": "python",
   "name": "emb_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
