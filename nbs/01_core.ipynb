{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from emb_opt.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Result\n",
    "\n",
    "The `QueryResult` serves as a data model to define the data required to integrate with the library. To integrate a new backend, you only need to figure out how to hook the returned information into the `QueryResult` class.\n",
    "\n",
    "* `query_idx` - query vectors are passed as a numpy array of size `(n, vector_size)`. This value corresponds to the index of the query vector that generated the specific result\n",
    "* `db_idx` - index value from the database\n",
    "* `embedding` - the embedding of the item\n",
    "* `distance` - the distance from the query vector to the embedding\n",
    "* `data` - a dictionary of any other values you want to track\n",
    "\n",
    "For more examples, see `HFDatabase`, `FaissDatabase`, `QdrantDatabase` or `ChromaDatabase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QueryResult():\n",
    "    def __init__(self, \n",
    "                 query_idx: int, # index of query vector\n",
    "                 db_idx: int, # index of item in database\n",
    "                 embedding: np.ndarray, # item embedding\n",
    "                 distance: float, # distance to query vector\n",
    "                 data: dict # any associated data\n",
    "                ):\n",
    "        self.query_idx = query_idx\n",
    "        self.db_idx = db_idx\n",
    "        self.embedding = embedding\n",
    "        self.distance = distance\n",
    "        self.data = data\n",
    "        \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'query_idx' : self.query_idx,\n",
    "            'db_idx' : self.db_idx,\n",
    "            'embedding' : self.embedding,\n",
    "            'distance' : self.distance,\n",
    "            'data' : self.data\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataset_from_query_results(query_results: list[QueryResult]) -> Dataset:\n",
    "    'generates a `Dataset` from a list of `QueryResult`'\n",
    "    data_dicts = [i.to_dict() for i in query_results]\n",
    "    return Dataset.from_list(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vecs = np.random.randn(2, 256)\n",
    "\n",
    "vector_database = np.random.randn(64, 256)\n",
    "\n",
    "topk = 24\n",
    "\n",
    "dists = ((query_vecs[:,None] - vector_database[None])**2).sum(-1)**0.5\n",
    "nearest = dists.argsort(-1)[:, -topk:]\n",
    "\n",
    "query_results = []\n",
    "\n",
    "for query_idx in range(query_vecs.shape[0]):\n",
    "    for db_idx in nearest[query_idx]:\n",
    "        result = QueryResult(query_idx, db_idx, vector_database[db_idx], \n",
    "                             dists[query_idx, db_idx], {'randint': np.random.randint(0,100)})\n",
    "        query_results.append(result)\n",
    "        \n",
    "query_dataset = dataset_from_query_results(query_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "The `Filter` class allows query results to be filtered by some function. See the [Huggingface Filter documentation](https://huggingface.co/docs/datasets/process#select-and-filter) for examples of filter functions and optional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Filter():\n",
    "    def __init__(self, \n",
    "                 filter_func: Callable, # function to filter\n",
    "                 filter_kwargs_dict: Optional[dict]=None # optional kwargs dict passed to `Dataset.filter`\n",
    "                ):\n",
    "        self.filter_func = filter_func\n",
    "        self.filter_kwargs_dict = filter_kwargs_dict if filter_kwargs_dict else {}\n",
    "        \n",
    "    def __call__(self, query_dataset: Dataset) -> Dataset:\n",
    "        return query_dataset.filter(lambda item: self.filter_func(item), **self.filter_kwargs_dict)\n",
    "    \n",
    "class PassThroughFilter(Filter):\n",
    "    'Dummy filter'\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, query_dataset: Dataset) -> Dataset:\n",
    "        return query_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def simple_filter(row):\n",
    "    return row['data']['randint'] < 20\n",
    "\n",
    "# basic filtering\n",
    "f = Filter(simple_filter)\n",
    "filtered_dataset = f(query_dataset)\n",
    "assert len(filtered_dataset) < len(query_dataset)\n",
    "\n",
    "# multiprocess\n",
    "f = Filter(simple_filter, {'num_proc':2})\n",
    "filtered_dataset = f(query_dataset)\n",
    "assert len(filtered_dataset) < len(query_dataset)\n",
    "\n",
    "# batched\n",
    "def batched_filter(batch):\n",
    "    randints = np.array([i['randint'] for i in batch['data']])\n",
    "    return randints < 20\n",
    "\n",
    "f = Filter(batched_filter, {'batched':True})\n",
    "filtered_dataset = f(query_dataset)\n",
    "assert len(filtered_dataset) < len(query_dataset)\n",
    "\n",
    "# dummy\n",
    "f = PassThroughFilter()\n",
    "filtered_dataset = f(query_dataset)\n",
    "assert len(filtered_dataset) == len(query_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score\n",
    "\n",
    "The `Score` class holds the score function we want to maximize. The score function will be given all the information in `QueryResult`. See [Huggingface Map Documentation](https://huggingface.co/docs/datasets/process#map) for available kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Score():\n",
    "    def __init__(self, \n",
    "                 score_func: Callable, # score function to maximize\n",
    "                 map_kwargs_dict: Optional[dict]=None # optional kwargs for `Dataset.map`\n",
    "                ):\n",
    "        self.score_func = score_func\n",
    "        self.map_kwargs_dict = map_kwargs_dict if map_kwargs_dict else {}\n",
    "        \n",
    "    def __call__(self, query_dataset: Dataset) -> Dataset:\n",
    "        \n",
    "        return query_dataset.map(lambda item: {'score' : self.score_func(item)}, **self.map_kwargs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def simple_score(row):\n",
    "    return np.random.randn()\n",
    "\n",
    "score = Score(simple_score)\n",
    "scored_dataset = score(query_dataset)\n",
    "\n",
    "# multiprocess\n",
    "score = Score(simple_score, {'num_proc' : 2})\n",
    "scored_dataset = score(query_dataset)\n",
    "\n",
    "# batched\n",
    "def batch_score(batch):\n",
    "    return np.arange(len(batch['query_idx']))\n",
    "\n",
    "score = Score(batch_score, {'batched' : True})\n",
    "scored_dataset = score(query_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VectorDatabase():\n",
    "    'Base class for vector database backends'\n",
    "    def query(self, query_vectors: np.ndarray) -> Dataset:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_opt",
   "language": "python",
   "name": "emb_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
