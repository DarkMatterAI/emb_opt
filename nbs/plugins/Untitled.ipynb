{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Plugins\n",
    "\n",
    "> Huggingface datasets functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugins.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from emb_opt.imports import *\n",
    "from emb_opt.schemas import Query, Item, DataSourceResponse\n",
    "from emb_opt.data_source import DataSourcePlugin, DataSourceModule\n",
    "from emb_opt.executor import Executor\n",
    "from emb_opt.utils import build_batch_from_embeddings\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DatasetExecutor(Executor):\n",
    "    '''\n",
    "    DatasetExecutor - executes function in parallel \n",
    "    using `Dataset.map`\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 function: Callable,              # function to be wrapped\n",
    "                 batched: bool,                   # if inputs should be batched\n",
    "                 batch_size: int=1,               # batch size (set batch_size=0 to pass all inputs)\n",
    "                 concurrency: Optional[int]=1,    # number of concurrent threads\n",
    "                 map_kwargs: Optional[dict]=None  # kwargs for `Dataset.map`\n",
    "                ):\n",
    "        \n",
    "        self.function = function\n",
    "        self.batched = batched\n",
    "        self.concurrency = concurrency\n",
    "        self.batch_size = batch_size\n",
    "        self.map_kwargs = map_kwargs if map_kwargs else {}\n",
    "        \n",
    "    def batch_inputs(self, inputs: List[BaseModel]):\n",
    "        dataset = datasets.Dataset.from_list([i.model_dump() for i in inputs])\n",
    "        return dataset\n",
    "            \n",
    "    def unbatch_inputs(self, dataset):\n",
    "        return dataset.to_list()\n",
    "\n",
    "    def execute(self, dataset):\n",
    "        dataset = dataset.map(lambda row: self.function(row), batched=self.batched, \n",
    "                             batch_size=self.batch_size, num_proc=self.concurrency, **self.map_kwargs)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "class TestInput(BaseModel):\n",
    "    value: float\n",
    "        \n",
    "class TestOutput(BaseModel):\n",
    "    result: bool\n",
    "\n",
    "def test_function_hf(input: dict) -> dict:\n",
    "    return {'result' : input['value']>0.5}\n",
    "\n",
    "def test_function_hf_batched(input: dict) -> dict:\n",
    "    return {'result' : [i>0.5 for i in input['value']]}\n",
    "        \n",
    "np.random.seed(42)\n",
    "values = np.random.uniform(size=100).tolist()\n",
    "inputs = [TestInput(value=i) for i in values]\n",
    "expected_outputs = [TestOutput(result=i>0.5) for i in values]\n",
    "\n",
    "# dataset\n",
    "\n",
    "executor = DatasetExecutor(test_function_hf, batched=False, concurrency=None, batch_size=1)\n",
    "res11 = executor(inputs)\n",
    "assert [TestOutput.model_validate(i) for i in res11] == expected_outputs\n",
    "\n",
    "executor = DatasetExecutor(test_function_hf, batched=False, concurrency=2, batch_size=1)\n",
    "res12 = executor(inputs)\n",
    "assert [TestOutput.model_validate(i) for i in res12] == expected_outputs\n",
    "\n",
    "executor = DatasetExecutor(test_function_hf_batched, batched=True, concurrency=2, batch_size=5)\n",
    "res13 = executor(inputs)\n",
    "assert [TestOutput.model_validate(i) for i in res13] == expected_outputs\n",
    "\n",
    "executor = DatasetExecutor(test_function_hf_batched, batched=True, concurrency=None, batch_size=5)\n",
    "res14 = executor(inputs)\n",
    "assert [TestOutput.model_validate(i) for i in res14] == expected_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class HugggingfaceDataPlugin(DataSourcePlugin):\n",
    "    '''\n",
    "    HugggingfaceDataPlugin - data plugin for working with \n",
    "    huggingface datasets library.\n",
    "    \n",
    "    The input `dataset` should have a faiss embedding index \n",
    "    denoted by `index_name`.\n",
    "    \n",
    "    The data query will run `k` nearest neighbors against the \n",
    "    dataset index based on the metric used to create the index\n",
    "    \n",
    "    Optionally, `item_key` denotes the column in `dataset` defining a \n",
    "    specific item, and `id_key` denotes the column defining an item's ID\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 k: int,                       # k nearest neighbors to return\n",
    "                 dataset: datasets.Dataset,    # input dataset\n",
    "                 index_name: str,              # name of the faiss index in `dataset`\n",
    "                 item_key: Optional[str]=None, # dataset column denoting item value\n",
    "                 id_key: Optional[str]=None    # dataset column denoting item id\n",
    "                ):\n",
    "        \n",
    "        self.k = k\n",
    "        self.dataset = dataset\n",
    "        self.index_name = index_name\n",
    "        self.index = self.dataset.get_index(index_name)\n",
    "        self.item_key = item_key\n",
    "        self.id_key = id_key\n",
    "        \n",
    "    def __call__(self, inputs: List[Query]) -> List[DataSourceResponse]:\n",
    "        queries = np.array([i.embedding for i in inputs])\n",
    "        \n",
    "        res = self.index.search_batch(queries, k=self.k)\n",
    "        distances = res.total_scores\n",
    "        indices = res.total_indices\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(indices.shape[0]):\n",
    "            items = []\n",
    "            query_data = {'query_distance' : []}\n",
    "            for j in range(indices.shape[1]):\n",
    "                query_data['query_distance'].append(distances[i,j])\n",
    "                \n",
    "                dataset_index = indices[i, j]\n",
    "                item_data = dict(self.dataset[int(dataset_index)])\n",
    "                embedding = item_data.pop(self.index_name)\n",
    "                item = item_data.pop(self.item_key) if self.item_key else None\n",
    "                item_id = item_data.pop(self.id_key) if self.id_key else None\n",
    "                \n",
    "                item = Item(id=item_id, \n",
    "                            item=item,\n",
    "                            embedding=embedding, \n",
    "                            data=item_data, \n",
    "                            score=None)\n",
    "                items.append(item)\n",
    "                \n",
    "            result = DataSourceResponse(valid=True, data=query_data, query_results=items)\n",
    "            outputs.append(result)\n",
    "            \n",
    "        return outputs       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1687.17it/s]\n"
     ]
    }
   ],
   "source": [
    "n_vectors = 256\n",
    "d_vectors = 64\n",
    "k = 10\n",
    "n_queries = 5\n",
    "\n",
    "vectors = np.random.randn(n_vectors, d_vectors)\n",
    "# vector_data = [{'index':np.random.randint(0,1e6), 'embedding':vectors[i]} \n",
    "#                for i in range(vectors.shape[0])]\n",
    "\n",
    "vector_data = [{'index':str(np.random.randint(0,1e6)), \n",
    "                'other':np.random.randint(0,1e3), \n",
    "                'item':str(np.random.randint(0,1e4)),\n",
    "                'embedding':vectors[i]\n",
    "               } \n",
    "               for i in range(vectors.shape[0])]\n",
    "\n",
    "dataset = Dataset.from_list(vector_data)\n",
    "dataset.add_faiss_index('embedding')\n",
    "\n",
    "data_function = HugggingfaceDataPlugin(k, dataset, 'embedding', 'item', 'index')\n",
    "data_module = DataSourceModule(data_function)\n",
    "\n",
    "batch = build_batch_from_embeddings(np.random.randn(n_queries, d_vectors))\n",
    "batch2 = data_module(batch)\n",
    "\n",
    "for q in batch2:\n",
    "    for r in q:\n",
    "        assert r.internal.parent_id == q.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_opt",
   "language": "python",
   "name": "emb_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
