{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# executor\n",
    "\n",
    "> executor functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/emb_opt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from emb_opt.imports import *\n",
    "from emb_opt.utils import batch_list, unbatch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executors\n",
    "\n",
    "`Executor` classes are helper classes for batching and parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Executor():\n",
    "    '''\n",
    "    Basic Executor class. Batches inputs, sends \n",
    "    batches to `function`, unbatches outputs\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 function: Callable, # function to be wrapped\n",
    "                 batched: bool,      # if inputs should be batched\n",
    "                 batch_size: int=1   # batch size (set batch_size=0 to pass all inputs)\n",
    "                ):\n",
    "        self.function = function\n",
    "        self.batched = batched\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def batch_inputs(self, inputs: List[BaseModel]):\n",
    "        if self.batched:\n",
    "            inputs = batch_list(inputs, self.batch_size)\n",
    "        return inputs\n",
    "            \n",
    "    def unbatch_inputs(self, results: List[BaseModel]):\n",
    "        if self.batched:\n",
    "            results = unbatch_list(results)\n",
    "        return results\n",
    "\n",
    "    def execute(self, inputs: List[BaseModel]):\n",
    "        results = [self.function(i) for i in inputs]\n",
    "        return results\n",
    "        \n",
    "    def __call__(self, inputs: List[BaseModel]) -> List[BaseModel]:\n",
    "        \n",
    "        inputs = self.batch_inputs(inputs)\n",
    "        results = self.execute(inputs)\n",
    "        results = self.unbatch_inputs(results)\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestInput(BaseModel):\n",
    "    value: float\n",
    "        \n",
    "class TestOutput(BaseModel):\n",
    "    result: bool\n",
    "        \n",
    "def test_function(input: TestInput) -> TestOutput:\n",
    "    return TestOutput(result=input.value>0.5)\n",
    "\n",
    "def test_function_batched(inputs: list[TestInput]) -> list[TestOutput]:\n",
    "    return [TestOutput(result=i.value>0.5) for i in inputs]\n",
    "        \n",
    "np.random.seed(42)\n",
    "values = np.random.uniform(size=100).tolist()\n",
    "inputs = [TestInput(value=i) for i in values]\n",
    "expected_outputs = [TestOutput(result=i>0.5) for i in values]\n",
    "\n",
    "# standard\n",
    "\n",
    "executor = Executor(test_function, batched=False)\n",
    "res1 = executor(inputs)\n",
    "assert res1 == expected_outputs\n",
    "\n",
    "executor = Executor(test_function_batched, batched=True, batch_size=5)\n",
    "res2 = executor(inputs)\n",
    "assert res2 == expected_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ProcessExecutor(Executor):\n",
    "    '''\n",
    "    ProcessExecutor - executes function with \n",
    "    multiprocessing using `ProcessPoolExecutor`\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 function: Callable,           # function to be wrapped\n",
    "                 batched: bool,                # if inputs should be batched\n",
    "                 batch_size: int=1,            # batch size (set batch_size=0 to pass all inputs)\n",
    "                 concurrency: Optional[int]=1  # number of concurrent processes\n",
    "                ):\n",
    "        \n",
    "        self.function = function\n",
    "        self.batched = batched\n",
    "        self.concurrency = concurrency\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def execute(self, inputs: List[BaseModel]):\n",
    "        if (self.concurrency is None) or (self.concurrency==1):\n",
    "            results = [self.function(i) for i in inputs]\n",
    "        else:\n",
    "            with ProcessPoolExecutor(min(self.concurrency, len(inputs))) as p:\n",
    "                results = list(p.map(self.function, inputs))\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestInput(BaseModel):\n",
    "    value: float\n",
    "        \n",
    "class TestOutput(BaseModel):\n",
    "    result: bool\n",
    "        \n",
    "def test_function(input: TestInput) -> TestOutput:\n",
    "    return TestOutput(result=input.value>0.5)\n",
    "\n",
    "def test_function_batched(inputs: list[TestInput]) -> list[TestOutput]:\n",
    "    return [TestOutput(result=i.value>0.5) for i in inputs]\n",
    "        \n",
    "np.random.seed(42)\n",
    "values = np.random.uniform(size=100).tolist()\n",
    "inputs = [TestInput(value=i) for i in values]\n",
    "expected_outputs = [TestOutput(result=i>0.5) for i in values]\n",
    "\n",
    "# process\n",
    "\n",
    "executor = ProcessExecutor(test_function, batched=False, concurrency=1)\n",
    "res3 = executor(inputs)\n",
    "assert res3 == expected_outputs\n",
    "\n",
    "executor = ProcessExecutor(test_function, batched=False, concurrency=2)\n",
    "res4 = executor(inputs)\n",
    "assert res4 == expected_outputs\n",
    "\n",
    "executor = ProcessExecutor(test_function_batched, batched=True, batch_size=5)\n",
    "res5 = executor(inputs)\n",
    "assert res5 == expected_outputs\n",
    "\n",
    "executor = ProcessExecutor(test_function_batched, batched=True, batch_size=5, concurrency=2)\n",
    "res6 = executor(inputs)\n",
    "assert res6 == expected_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ThreadExecutor(Executor):\n",
    "    '''\n",
    "    ProcessExecutor - executes function with \n",
    "    multiple threads using `ThreadPoolExecutor`\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 function: Callable,           # function to be wrapped\n",
    "                 batched: bool,                # if inputs should be batched\n",
    "                 batch_size: int=1,            # batch size (set batch_size=0 to pass all inputs)\n",
    "                 concurrency: Optional[int]=1  # number of concurrent threads\n",
    "                ):\n",
    "        \n",
    "        self.function = function\n",
    "        self.batched = batched\n",
    "        self.concurrency = concurrency\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def execute(self, inputs: List[BaseModel]):\n",
    "        if (self.concurrency is None) or (self.concurrency==1):\n",
    "            results = [self.function(i) for i in inputs]\n",
    "        else:\n",
    "            with ThreadPoolExecutor(min(self.concurrency, len(inputs))) as p:\n",
    "                results = list(p.map(self.function, inputs))\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestInput(BaseModel):\n",
    "    value: float\n",
    "        \n",
    "class TestOutput(BaseModel):\n",
    "    result: bool\n",
    "        \n",
    "def test_function(input: TestInput) -> TestOutput:\n",
    "    return TestOutput(result=input.value>0.5)\n",
    "\n",
    "def test_function_batched(inputs: list[TestInput]) -> list[TestOutput]:\n",
    "    return [TestOutput(result=i.value>0.5) for i in inputs]\n",
    "        \n",
    "np.random.seed(42)\n",
    "values = np.random.uniform(size=100).tolist()\n",
    "inputs = [TestInput(value=i) for i in values]\n",
    "expected_outputs = [TestOutput(result=i>0.5) for i in values]\n",
    "\n",
    "# thread\n",
    "\n",
    "executor = ThreadExecutor(test_function, batched=False, concurrency=1)\n",
    "res7 = executor(inputs)\n",
    "assert res7 == expected_outputs\n",
    "\n",
    "executor = ThreadExecutor(test_function, batched=False, concurrency=2)\n",
    "res8 = executor(inputs)\n",
    "assert res8 == expected_outputs\n",
    "\n",
    "executor = ThreadExecutor(test_function_batched, batched=True, batch_size=5)\n",
    "res9 = executor(inputs)\n",
    "assert res9 == expected_outputs\n",
    "\n",
    "executor = ThreadExecutor(test_function_batched, batched=True, batch_size=5, concurrency=2)\n",
    "res10 = executor(inputs)\n",
    "assert res10 == expected_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DatasetExecutor(Executor):\n",
    "    '''\n",
    "    DatasetExecutor - executes function in parallel \n",
    "    using `Dataset.map`\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 function: Callable,              # function to be wrapped\n",
    "                 batched: bool,                   # if inputs should be batched\n",
    "                 batch_size: int=1,               # batch size (set batch_size=0 to pass all inputs)\n",
    "                 concurrency: Optional[int]=1,    # number of concurrent threads\n",
    "                 map_kwargs: Optional[dict]=None  # kwargs for `Dataset.map`\n",
    "                ):\n",
    "        \n",
    "        self.function = function\n",
    "        self.batched = batched\n",
    "        self.concurrency = concurrency\n",
    "        self.batch_size = batch_size\n",
    "        self.map_kwargs = map_kwargs if map_kwargs else {}\n",
    "        \n",
    "    def batch_inputs(self, inputs: List[BaseModel]):\n",
    "        dataset = datasets.Dataset.from_list([i.model_dump() for i in inputs])\n",
    "        return dataset\n",
    "            \n",
    "    def unbatch_inputs(self, dataset):\n",
    "        return dataset.to_list()\n",
    "\n",
    "    def execute(self, dataset):\n",
    "        dataset = dataset.map(lambda row: self.function(row), batched=self.batched, \n",
    "                             batch_size=self.batch_size, num_proc=self.concurrency, **self.map_kwargs)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "class TestInput(BaseModel):\n",
    "    value: float\n",
    "        \n",
    "class TestOutput(BaseModel):\n",
    "    result: bool\n",
    "\n",
    "def test_function_hf(input: dict) -> dict:\n",
    "    return {'result' : input['value']>0.5}\n",
    "\n",
    "def test_function_hf_batched(input: dict) -> dict:\n",
    "    return {'result' : [i>0.5 for i in input['value']]}\n",
    "        \n",
    "np.random.seed(42)\n",
    "values = np.random.uniform(size=100).tolist()\n",
    "inputs = [TestInput(value=i) for i in values]\n",
    "expected_outputs = [TestOutput(result=i>0.5) for i in values]\n",
    "\n",
    "# dataset\n",
    "\n",
    "executor = DatasetExecutor(test_function_hf, batched=False, concurrency=None, batch_size=1)\n",
    "res11 = executor(inputs)\n",
    "assert [TestOutput.model_validate(i) for i in res11] == expected_outputs\n",
    "\n",
    "executor = DatasetExecutor(test_function_hf, batched=False, concurrency=2, batch_size=1)\n",
    "res12 = executor(inputs)\n",
    "assert [TestOutput.model_validate(i) for i in res12] == expected_outputs\n",
    "\n",
    "executor = DatasetExecutor(test_function_hf_batched, batched=True, concurrency=2, batch_size=5)\n",
    "res13 = executor(inputs)\n",
    "assert [TestOutput.model_validate(i) for i in res13] == expected_outputs\n",
    "\n",
    "executor = DatasetExecutor(test_function_hf_batched, batched=True, concurrency=None, batch_size=5)\n",
    "res14 = executor(inputs)\n",
    "assert [TestOutput.model_validate(i) for i in res14] == expected_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_opt",
   "language": "python",
   "name": "emb_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
