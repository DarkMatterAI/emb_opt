{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from emb_opt.imports import *\n",
    "from emb_opt.schemas import Batch, Query, Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def batch_list(inputs: List[Any],   # input list to be batched\n",
    "               batch_size: int      # batch size\n",
    "              ) -> List[List[Any]]: # batched output list\n",
    "    '''\n",
    "    batches the input list into chunks of size `batch_size`, with the last batch ragged\n",
    "    \n",
    "    if `batch_size=0`, returns list of all inputs\n",
    "    '''\n",
    "    if batch_size==0:\n",
    "        output = [inputs]\n",
    "    else:\n",
    "        output = [inputs[i:i+batch_size] for i in range(0, len(inputs), batch_size)]\n",
    "    return output\n",
    "\n",
    "def unbatch_list(inputs: List[List[Any]] # input batched list\n",
    "                ) -> List[Any]:          # flattened output list\n",
    "    'flattens a batched list'\n",
    "    return [item for sublist in inputs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = list(range(10))\n",
    "assert unbatch_list(batch_list(inputs, 3)) == inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def build_batch_from_embeddings(embeddings: List[List[float]] # input embeddings\n",
    "                               ) -> Batch:                    # output batch\n",
    "    '''\n",
    "    creates a `Batch` from a list of `embeddings`. Each embedding \n",
    "    is converted to a `Query` with a unique `collection_id`\n",
    "    '''\n",
    "    queries = []\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        query = Query.from_minimal(embedding=embedding)\n",
    "        query.update_internal(collection_id=i)\n",
    "        queries.append(query)\n",
    "        \n",
    "    batch = Batch(queries=queries) \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(queries=[Query(item=None, embedding=[0.1], data={}, query_results=[], internal=InteralData(removed=False, removal_reason=None, parent_id=None, collection_id=0, iteration=None), id='query_191d47ea-5809-11ee-b05f-db94e348bdfb'), Query(item=None, embedding=[0.2], data={}, query_results=[], internal=InteralData(removed=False, removal_reason=None, parent_id=None, collection_id=1, iteration=None), id='query_191d47eb-5809-11ee-b05f-db94e348bdfb')])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_batch_from_embeddings([[0.1], [0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def build_batch_from_items(items: List[Item],      # input items\n",
    "                           remap_collections=False # if collection ID should be remapped\n",
    "                          ) -> Batch:              # output batch\n",
    "    '''\n",
    "    creates a `Batch` from a list of `Item` objects. Each `Item` \n",
    "    is converted to a `Query`. If `remap_collections=True`, each \n",
    "    `Query` is given a unique `collection_id`. Otherwise, each \n",
    "    `Query` retains the `collection_id` of the `Item` used to \n",
    "    create it\n",
    "    '''\n",
    "    queries = []\n",
    "    for i, item in enumerate(items):\n",
    "        query = Query.from_item(item)\n",
    "        if remap_collections:\n",
    "            query.update_internal(collection_id=i)\n",
    "        queries.append(query)\n",
    "    batch = Batch(queries=queries) \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(queries=[Query(item=None, embedding=[0.1], data={'_source_item_id': 'item_191d47ec-5809-11ee-b05f-db94e348bdfb'}, query_results=[], internal=InteralData(removed=False, removal_reason=None, parent_id=None, collection_id=0, iteration=None), id='query_191d47ed-5809-11ee-b05f-db94e348bdfb')])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_batch_from_items([Item.from_minimal(embedding=[0.1])], remap_collections=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def whiten(scores: np.ndarray # vector shape (n,) of scores to whiten\n",
    "          ) -> np.ndarray:    # vector shape (n,) whitened scores\n",
    "    'Whitens vector of scores'\n",
    "    mean = scores.mean()\n",
    "    var = scores.var()\n",
    "    \n",
    "    return (scores - mean) / np.sqrt(var + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def clip_grad(grad: np.ndarray,                  # grad vector\n",
    "              max_norm: float,                   # max grad norm\n",
    "              norm_type: Union[float, int, str]  # type of norm to use\n",
    "             ):\n",
    "    current_norm = np.linalg.norm(grad, ord=norm_type, axis=-1, keepdims=True)\n",
    "    coef = np.minimum(max_norm / (current_norm + 1e-6), 1)\n",
    "    grad = grad * coef\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.array([1, 2, 3, 4, 5])\n",
    "grads = np.stack([grad, grad])\n",
    "assert (clip_grad(grad, 1., 2) == clip_grad(grads, 1., 2)[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def query_to_rl_inputs(query: Query) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    query_embedding = np.array(query.embedding)\n",
    "    result_embeddings = np.array([i.embedding for i in query.valid_results()])\n",
    "    scores = np.array([i.score for i in query.valid_results()])\n",
    "    return query_embedding, result_embeddings, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def compute_rl_grad(query_embeddings: np.ndarray,                     # matrix of query embeddings\n",
    "                    result_embeddings: np.ndarray,                    # matrix of result embeddings\n",
    "                    result_scores: np.ndarray,                        # array of scores\n",
    "                    distance_penalty: float=0,                        # distance penalty coefficient\n",
    "                    max_norm: Optional[float]=None,                   # max gradient norm\n",
    "                    norm_type: Optional[Union[float, int, str]]=2.0,  # type of norm to use\n",
    "                    score_grad=False                                  # if loss should be score grad or loss grad\n",
    "                   ):\n",
    "    \n",
    "    '''\n",
    "    compute_rl_grad - uses reinforcement learning to estimate query gradients\n",
    "    \n",
    "    To compute the gradient with RL:\n",
    "    1. compute advantages by whitening scores\n",
    "        1. `advantage[i] = (scores[i] - scores.mean()) / scores.std()`\n",
    "    2. compute advantage loss\n",
    "        1. `advantage_loss[i] = advantage[i] * (query_embedding - result_embedding[i])**2`\n",
    "    3. compute distance loss\n",
    "        1. `distance_loss[i] = distance_penalty * (query_embedding - result_embedding[i])**2`\n",
    "    4. sum loss terms\n",
    "        1. `loss[i] = advantage_loss[i] + distance_loss[i]`\n",
    "    5. compute the gradient\n",
    "    \n",
    "    This gives a closed for calculation of the gradient as:\n",
    "    \n",
    "    `grad[i] = 2 * (advantage[i] + distance_penalty) * (query_embedding - result_embedding[i])` \n",
    "    \n",
    "    if `max_norm` is specified, the gradient will be clipped using `norm_type`\n",
    "    \n",
    "    if `score_grad=True`, the sign of the gradient is flipped. The standard sign is \n",
    "    designed for _minimizing_ the loss via gradient descent via `n_new = n_old - lr * grad`. \n",
    "    With the sign flipped, the gradient points directly in the direction of increasing score, \n",
    "    which is conceptually aligned with hill climbing, updating via `n_new = n_old + lr * grad`. \n",
    "    Use `score_grad=False` for anything using gradient descent.\n",
    "    '''\n",
    "    \n",
    "    if query_embeddings.ndim==1:\n",
    "        query_embeddings = query_embeddings[None]\n",
    "        \n",
    "    if result_embeddings.ndim==1:\n",
    "        result_embeddings = result_embeddings[None]\n",
    "        \n",
    "    advantages = whiten(result_scores) + distance_penalty\n",
    "    \n",
    "    query_distances = query_embeddings[:,None,:] - result_embeddings[None,:,:]\n",
    "    \n",
    "    grad = (2 * advantages[None, :, None] * query_distances).mean(1)\n",
    "    \n",
    "    if max_norm is not None:\n",
    "        assert norm_type is not None\n",
    "        grad = clip_grad(grad, max_norm, norm_type)\n",
    "        \n",
    "    if score_grad:\n",
    "        grad = -grad\n",
    "    \n",
    "    return np.squeeze(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_opt",
   "language": "python",
   "name": "emb_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
